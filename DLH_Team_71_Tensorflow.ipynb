{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfyi6FdlRDyg"
      },
      "source": [
        "# CS598 Deep Learning for Healthcare - Final Project - DeepMicro: Deep Representation Learning for Disease Prediction based on Microbiome Data\n",
        "\n",
        "### Lotte Zhu, Kaiqing Zhang, Matthew Trueblood (Team ID: 71)\n",
        "#### GitHub Link: https://github.com/ZKingQ/CS598-DLH-SP24"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j01aH0PR4Sg-"
      },
      "source": [
        "# Before you use this template\n",
        "\n",
        "This template is just a recommended template for project Report. It only considers the general type of research in our paper pool. Feel free to edit it to better fit your project. You will iteratively update the same notebook submission for your draft and the final submission. Please check the project rubriks to get a sense of what is expected in the template.\n",
        "\n",
        "---\n",
        "\n",
        "# FAQ and Attentions\n",
        "* Copy and move this template to your Google Drive. Name your notebook by your team ID (upper-left corner). Don't eidt this original file.\n",
        "* This template covers most questions we want to ask about your reproduction experiment. You don't need to exactly follow the template, however, you should address the questions. Please feel free to customize your report accordingly.\n",
        "* any report must have run-able codes and necessary annotations (in text and code comments).\n",
        "* The notebook is like a demo and only uses small-size data (a subset of original data or processed data), the entire runtime of the notebook including data reading, data process, model training, printing, figure plotting, etc,\n",
        "must be within 8 min, otherwise, you may get penalty on the grade.\n",
        "  * If the raw dataset is too large to be loaded  you can select a subset of data and pre-process the data, then, upload the subset or processed data to Google Drive and load them in this notebook.\n",
        "  * If the whole training is too long to run, you can only set the number of training epoch to a small number, e.g., 3, just show that the training is runable.\n",
        "  * For results model validation, you can train the model outside this notebook in advance, then, load pretrained model and use it for validation (display the figures, print the metrics).\n",
        "* The post-process is important! For post-process of the results,please use plots/figures. The code to summarize results and plot figures may be tedious, however, it won't be waste of time since these figures can be used for presentation. While plotting in code, the figures should have titles or captions if necessary (e.g., title your figure with \"Figure 1. xxxx\")\n",
        "* There is not page limit to your notebook report, you can also use separate notebooks for the report, just make sure your grader can access and run/test them.\n",
        "* If you use outside resources, please refer them (in any formats). Include the links to the resources if necessary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlv6knX04FiY"
      },
      "source": [
        "# Mount Notebook to Google Drive\n",
        "Upload the data, pretrianed model, figures, etc to your Google Drive, then mount this notebook to Google Drive. After that, you can access the resources freely.\n",
        "\n",
        "Instruction: https://colab.research.google.com/notebooks/io.ipynb\n",
        "\n",
        "Example: https://colab.research.google.com/drive/1srw_HFWQ2SMgmWIawucXfusGzrj1_U0q\n",
        "\n",
        "Video: https://www.youtube.com/watch?v=zc8g8lGcwQU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gL7PJw2sGb3",
        "outputId": "339de7d4-8320-4937-eb60-a7aee8e7c4e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ0sNuMePBXx"
      },
      "source": [
        "# Introduction\n",
        "<!-- This is an introduction to your report, you should edit this text/mardown section to compose. In this text/markdown, you should introduce:\n",
        "\n",
        "*   Background of the problem\n",
        "  * what type of problem: disease/readmission/mortality prediction,  feature engineeing, data processing, etc\n",
        "  * what is the importance/meaning of solving the problem\n",
        "  * what is the difficulty of the problem\n",
        "  * the state of the art methods and effectiveness.\n",
        "*   Paper explanation\n",
        "  * what did the paper propose\n",
        "  * what is the innovations of the method\n",
        "  * how well the proposed method work (in its own metrics)\n",
        "  * what is the contribution to the reasearch regime (referring the Background above, how important the paper is to the problem). -->\n",
        "\n",
        "\n",
        "The expanding knowledge of microbiota uncovers its crucial role in human health [1]. It plays an important role in immune system, metabolism functions and even carcinognesis of certain cancers, hence microbiota can be used to predict various disease with emerging sequencing technologies [1,2,3,4,5,6]. However, there are three major challenges to realize the predictions in practice [7]. First, the low number of samples, together with the large number of features, leads to the curse of dimensionality. Second, there is a research gap in using strain-level profiles to classify samples into patient and healthy control groups across different diseases. Third, a rigorous validation framework is essential. Prior research has shown that tuning hyperparameters on the test set without a separate validation set may lead to an overestimation of model performance [8,9,10].\n",
        "\n",
        "The DeepMicro paper proposes the deployment of autoencoders to learn low-dimensional representations from microbiota data and then predict disease by another classification model based on the learned representations, with both trainings having thorough validation schemes [7]. The authors hypothesize that these innovations can contribute to the followings:\n",
        "\n",
        "1. The appropriate autoencoders can effectively solve the curse of dimensionality.\n",
        "\n",
        "2. They also reduce latency compared with alternative models without representation learing, in the mean time maintaining favorable training metrics.\n",
        "\n",
        "In this project, our primary objective revolves around the implementation and evaluation of various autoencoder architectures. We aim to validate the disease prediction capabilities as outlined in the paper. Furthermore, we discuss the ablations inherent in the innovation of autoencoder. The rationale for choosing this paper stems from our pursuit of knowledge in deep representation learning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABD4VhFZbehA"
      },
      "outputs": [],
      "source": [
        "# code comment is used as inline annotations for your coding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uygL9tTPSVHB"
      },
      "source": [
        "# Scope of Reproducibility:\n",
        "\n",
        "<!-- List hypotheses from the paper you will test and the corresponding experiments you will run.\n",
        "\n",
        "\n",
        "1.   Hypothesis 1: xxxxxxx\n",
        "2.   Hypothesis 2: xxxxxxx\n",
        "\n",
        "You can insert images in this notebook text, [see this link](https://stackoverflow.com/questions/50670920/how-to-insert-an-inline-image-in-google-colaboratory-from-google-drive) and example below:\n",
        "\n",
        "![sample_image.png](https://drive.google.com/uc?export=view&id=1g2efvsRJDxTxKz-OY3loMhihrEUdBxbc)\n",
        "\n",
        "\n",
        "You can also use code to display images, see the code below.\n",
        "\n",
        "The images must be saved in Google Drive first.\n",
        "\n",
        "-->\n",
        "\n",
        "Our team achieved successful replication of the model by leveraging its open source codebase, resulting in metrics that are comparable to those showcased in the DeepMicro paper. In our endeavor, we thoroughly examined and addressed the claims below put forth in the original paper.\n",
        "\n",
        "1. **Dimensionality reduction engineering with traditional statistical techniques including Principal Component Analysis (PCA) and Gaussian Random Projection (GRP)**\n",
        "\n",
        "- Principal Component Analysis (PCA) aims to capture the most significant patterns and variations in a dataset by identifying orthogonal axes, known as principal components, that maximize the data variance.\n",
        "- Gaussian Random Projection (GRP) seeks to preserve pairwise distances between data points by projecting them onto a lower-dimensional space using random projections drawn from a Gaussian distribution.\n",
        "\n",
        "2. **Innovated representation learning employing with four different autoencoders including Shallow Autoencoder (SAE), Deep Autoencoder (DAE), Variational Autoencoder (VAE), and Convolutional Autoencoder (CAE)**\n",
        "\n",
        "- Shallow Autoencoder (SAE): This is the simplest form of an autoencoder, consisting of a fully\n",
        "connected encoder layer and a decoder layer. The latent representation is obtained from the encoder\n",
        "layer, which is a lower-dimensional space compared to the original input.\n",
        "- Deep Autoencoder (DAE): In addition to the encoder and decoder layers, DAE introduces hidden\n",
        "layers between the input and latent layers and between the latent and output layers. Rectified Linear\n",
        "Unit (ReLU) activation functions are used in the hidden layers.\n",
        "- Variational Autoencoder (VAE): VAE learns probabilistic representations by approximating the\n",
        "true posterior distribution of latent embeddings. It assumes that the posterior distribution follows\n",
        "a Gaussian distribution. VAE uses an encoder network to encode the means and variances of the\n",
        "Gaussian distribution and samples the latent representation from this distribution. The decoder\n",
        "network then reconstructs the input based on the sampled latent representation.\n",
        "- Convolutional Autoencoder (CAE): Instead of fully connected layers, CAE incorporates convo-\n",
        "lutional layers, where each unit is connected to local regions of the previous layer. Convolutional layers use filters (kernels) to perform convolution operations. CAE employs convolutional transpose layers (deconvolutional layers) to make the decoder symmetric to the encoder. No pooling layers\n",
        "are used in CAE.\n",
        "\n",
        "3. **Classification learning including including Support Vector Machine (SVM), Random Forest (RF), and Multi-Layer Perceptron (MLP)**\n",
        "- Support Vector Machine (SVM) is a supervised learning algorithm that aims to find an optimal hyperplane to classify data by maximizing the margin between different classes.\n",
        "- Random Forest (RF) is an ensemble learning method that constructs a multitude of decision trees and combines their predictions to make classifications.\n",
        "- Multi-Layer Perceptron (MLP) is a type of neural network that consists of multiple layers of interconnected nodes, enabling it to learn non-linear relationships and perform classification tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRksCB1vbYwJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      },
      "source": [
        "# Methodology\n",
        "\n",
        "This methodology is the core of your project. It consists of run-able codes with necessary annotations to show the expeiment you executed for testing the hypotheses.\n",
        "\n",
        "The methodology at least contains two subsections **data** and **model** in your experiment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPov0WCKWbPh"
      },
      "source": [
        "## Environment Set Up and Packages Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTXyVLh8YFl7",
        "outputId": "a85e601a-c82d-470f-9d32-3d6454378136"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ],
      "source": [
        "# Python version\n",
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMEQdI728g55",
        "outputId": "9eb8857b-dbc6-489e-f8f4-7e4f2b46947f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==2.12.0 in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (24.3.25)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.62.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (3.9.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (0.4.26)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (18.1.1)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (0.36.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.12.0) (0.43.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.2.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (1.11.4)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.2.2)\n",
            "Requirement already satisfied: keras==2.12.0 in /usr/local/lib/python3.10/dist-packages (2.12.0)\n"
          ]
        }
      ],
      "source": [
        "# set up tensorflow env\n",
        "# !pip uninstall tensorflow\n",
        "!pip install tensorflow==2.12.0\n",
        "!pip install keras==2.12.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Lyf5Jfp7sGWw"
      },
      "outputs": [],
      "source": [
        "# import packages needed in this project\n",
        "import numpy as np\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import datetime\n",
        "import math\n",
        "import matplotlib\n",
        "matplotlib.use('agg')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.random_projection import GaussianRandomProjection\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# importing keras\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, LambdaCallback\n",
        "from keras.models import Model, load_model, Sequential\n",
        "from keras.layers import Dense, Dropout, Input, Lambda, Conv2D, Conv2DTranspose, MaxPool2D, UpSampling2D, Flatten, Reshape, Cropping2D\n",
        "from keras import backend as K\n",
        "from keras.losses import mse, binary_crossentropy\n",
        "\n",
        "from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NbPHUTMbkD3"
      },
      "source": [
        "##  Data\n",
        "Data includes raw data (MIMIC III tables), descriptive statistics (our homework questions), and data processing (feature engineering).\n",
        "  * Source of the data: where the data is collected from; if data is synthetic or self-generated, explain how. If possible, please provide a link to the raw datasets.\n",
        "  * Statistics: include basic descriptive statistics of the dataset like size, cross validation split, label distribution, etc.\n",
        "  * Data process: how do you munipulate the data, e.g., change the class labels, split the dataset to train/valid/test, refining the dataset.\n",
        "  * Illustration: printing results, plotting figures for illustration.\n",
        "  * You can upload your raw dataset to Google Drive and mount this Colab to the same directory. If your raw dataset is too large, you can upload the processed dataset and have a code to load the processed dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrtmVFrvcW6p"
      },
      "source": [
        "### Data Path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRRY_dsgYL2Q"
      },
      "source": [
        "We downloaded two datasets, abundance and marker from the [DeepMicro codebase](https://github.com/minoh0201/DeepMicro/tree/master/data). They are stored in the following path on Google Drive:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yGCbNWqfcRcs"
      },
      "outputs": [],
      "source": [
        "# data dir\n",
        "raw_data_dir = '/content/drive/My Drive/Colab Notebooks/data/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJNbhZZ7cTjS"
      },
      "source": [
        "### Data Description\n",
        "\n",
        "Our reproductivity utilizes the same datasets as the original paper, which include six disease (Table 1). They are inflammatory bowel disease (IBD), type 2 diabetes in European women (EW-T2D), type 2 diabetes in Chinese (C-T2D), obesity (Obesity), liver cirrhosis (Cirrhosis), and colorectal cancer (Colorectal).\n",
        "![](assets/Data_Table1.png)\n",
        "\n",
        "In each dataset, marker profile and abundance profile of microbiome are used to train our models (Table 2).\n",
        "![](assets/Data_Table2.png)\n",
        "\n",
        "All the data are stored in txt format and the data path structure is as following.\n",
        "```\n",
        ".\n",
        "├── ...\n",
        "├── data                              # Data folder\n",
        "│   ├── marker                        # Marker profile data\n",
        "│       ├── marker_IBD.txt            # Inflammatory bowel disease (IBD)\n",
        "│       ├── marker_WT2D.txt           # Type 2 diabetes in European women (EW-T2D)\n",
        "│       ├── marker_T2D.txt            # Type 2 diabetes in Chinese (C-T2D)\n",
        "│       ├── marker_Obesity.txt        # Obesity (Obesity)\n",
        "│       ├── marker_Cirrhosis.txt      # Liver cirrhosis (Cirrhosis)\n",
        "│       ├── marker_Colorectal.txt     # Colorectal cancer (Colorectal)\n",
        "│   ├── abundance                     # Abundance profile data\n",
        "│       ├── abundance_IBD.txt         # Inflammatory bowel disease (IBD)\n",
        "│       ├── abundance_WT2D.txt        # Type 2 diabetes in European women (EW-T2D)\n",
        "│       ├── abundance_T2D.txt         # Type 2 diabetes in Chinese (C-T2D)\n",
        "│       ├── abundance_Obesity.txt     # Obesity (Obesity)\n",
        "│       ├── abundance_Cirrhosis.txt   # Liver cirrhosis (Cirrhosis)\n",
        "│       ├── abundance_Colorectal.txt  # Colorectal cancer (Colorectal)\n",
        "└── ...\n",
        "```\n",
        "In each txt file, it has different number of features and data points as shown in Table 1 and Table 2 above. In the sample demonstration below. The colorectal cancer of microbiome abundance profile has 503 features (exluding those dummy labels) and 121 entries.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "id": "BYGSgWXzjdeX",
        "outputId": "6a9a07d6-cbf6-4adc-9175-f5caf2d58864"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "sample"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-6a86ed72-a06e-4462-9c19-7603a2c307b8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dataset_name</th>\n",
              "      <th>sampleID</th>\n",
              "      <th>subjectID</th>\n",
              "      <th>bodysite</th>\n",
              "      <th>disease</th>\n",
              "      <th>age</th>\n",
              "      <th>gender</th>\n",
              "      <th>country</th>\n",
              "      <th>sequencing_technology</th>\n",
              "      <th>pubmedid</th>\n",
              "      <th>...</th>\n",
              "      <th>k__Eukaryota|p__Ascomycota|c__Saccharomycetes|o__Saccharomycetales|f__Saccharomycetaceae|g__Eremothecium|s__Eremothecium_unclassified</th>\n",
              "      <th>k__Bacteria|p__Firmicutes|c__Bacilli|o__Lactobacillales|f__Lactobacillaceae|g__Lactobacillus|s__Lactobacillus_antri</th>\n",
              "      <th>k__Bacteria|p__Firmicutes|c__Bacilli|o__Bacillales|f__Bacillaceae|g__Lysinibacillus|s__Lysinibacillus_fusiformis</th>\n",
              "      <th>k__Archaea|p__Euryarchaeota|c__Methanobacteria|o__Methanobacteriales|f__Methanobacteriaceae|g__Methanobacterium|s__Methanobacterium_unclassified</th>\n",
              "      <th>k__Bacteria|p__Firmicutes|c__Bacilli|o__Bacillales|f__Bacillaceae|g__Lysinibacillus|s__Lysinibacillus_boronitolerans</th>\n",
              "      <th>k__Bacteria|p__Firmicutes|c__Bacilli|o__Lactobacillales|f__Enterococcaceae|g__Bavariicoccus|s__Bavariicoccus_seileri</th>\n",
              "      <th>k__Bacteria|p__Firmicutes|c__Bacilli|o__Lactobacillales|f__Enterococcaceae|g__Enterococcus|s__Enterococcus_gilvus</th>\n",
              "      <th>k__Bacteria|p__Firmicutes|c__Bacilli|o__Lactobacillales|f__Lactobacillaceae|g__Lactobacillus|s__Lactobacillus_otakiensis</th>\n",
              "      <th>k__Bacteria|p__Firmicutes|c__Clostridia|o__Clostridiales|f__Peptococcaceae|g__Desulfotomaculum|s__Desulfotomaculum_ruminis</th>\n",
              "      <th>k__Bacteria|p__Firmicutes|c__Negativicutes|o__Selenomonadales|f__Veillonellaceae|g__Megasphaera|s__Megasphaera_sp_BV3C16_1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Zeller_fecal_colorectal_cancer</td>\n",
              "      <td>CCIS00146684ST-4-0</td>\n",
              "      <td>fr-726</td>\n",
              "      <td>stool</td>\n",
              "      <td>n</td>\n",
              "      <td>72</td>\n",
              "      <td>female</td>\n",
              "      <td>france</td>\n",
              "      <td>Illumina</td>\n",
              "      <td>25432777</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Zeller_fecal_colorectal_cancer</td>\n",
              "      <td>CCIS00281083ST-3-0</td>\n",
              "      <td>fr-060</td>\n",
              "      <td>stool</td>\n",
              "      <td>n</td>\n",
              "      <td>53</td>\n",
              "      <td>male</td>\n",
              "      <td>france</td>\n",
              "      <td>Illumina</td>\n",
              "      <td>25432777</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Zeller_fecal_colorectal_cancer</td>\n",
              "      <td>CCIS02124300ST-4-0</td>\n",
              "      <td>fr-568</td>\n",
              "      <td>stool</td>\n",
              "      <td>n</td>\n",
              "      <td>35</td>\n",
              "      <td>male</td>\n",
              "      <td>france</td>\n",
              "      <td>Illumina</td>\n",
              "      <td>25432777</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Zeller_fecal_colorectal_cancer</td>\n",
              "      <td>CCIS02379307ST-4-0</td>\n",
              "      <td>fr-828</td>\n",
              "      <td>stool</td>\n",
              "      <td>cancer</td>\n",
              "      <td>67</td>\n",
              "      <td>male</td>\n",
              "      <td>france</td>\n",
              "      <td>Illumina</td>\n",
              "      <td>25432777</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Zeller_fecal_colorectal_cancer</td>\n",
              "      <td>CCIS03473770ST-4-0</td>\n",
              "      <td>fr-192</td>\n",
              "      <td>stool</td>\n",
              "      <td>n</td>\n",
              "      <td>29</td>\n",
              "      <td>male</td>\n",
              "      <td>france</td>\n",
              "      <td>Illumina</td>\n",
              "      <td>25432777</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.03756</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Zeller_fecal_colorectal_cancer</td>\n",
              "      <td>CCIS06260551ST-3-0</td>\n",
              "      <td>fr-200</td>\n",
              "      <td>stool</td>\n",
              "      <td>cancer</td>\n",
              "      <td>58</td>\n",
              "      <td>male</td>\n",
              "      <td>france</td>\n",
              "      <td>Illumina</td>\n",
              "      <td>25432777</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.31121</td>\n",
              "      <td>0</td>\n",
              "      <td>0.03562</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Zeller_fecal_colorectal_cancer</td>\n",
              "      <td>CCIS07539127ST-4-0</td>\n",
              "      <td>fr-460</td>\n",
              "      <td>stool</td>\n",
              "      <td>n</td>\n",
              "      <td>77</td>\n",
              "      <td>female</td>\n",
              "      <td>france</td>\n",
              "      <td>Illumina</td>\n",
              "      <td>25432777</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Zeller_fecal_colorectal_cancer</td>\n",
              "      <td>CCIS07648107ST-4-0</td>\n",
              "      <td>fr-053</td>\n",
              "      <td>stool</td>\n",
              "      <td>n</td>\n",
              "      <td>62</td>\n",
              "      <td>female</td>\n",
              "      <td>france</td>\n",
              "      <td>Illumina</td>\n",
              "      <td>25432777</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Zeller_fecal_colorectal_cancer</td>\n",
              "      <td>CCIS08668806ST-3-0</td>\n",
              "      <td>fr-214</td>\n",
              "      <td>stool</td>\n",
              "      <td>small_adenoma</td>\n",
              "      <td>63</td>\n",
              "      <td>male</td>\n",
              "      <td>france</td>\n",
              "      <td>Illumina</td>\n",
              "      <td>25432777</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Zeller_fecal_colorectal_cancer</td>\n",
              "      <td>CCIS09568613ST-4-0</td>\n",
              "      <td>fr-400</td>\n",
              "      <td>stool</td>\n",
              "      <td>n</td>\n",
              "      <td>67</td>\n",
              "      <td>male</td>\n",
              "      <td>france</td>\n",
              "      <td>Illumina</td>\n",
              "      <td>25432777</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 714 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6a86ed72-a06e-4462-9c19-7603a2c307b8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6a86ed72-a06e-4462-9c19-7603a2c307b8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6a86ed72-a06e-4462-9c19-7603a2c307b8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a2f5ee6e-bfb1-4813-93aa-1af8b2b75bbd\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a2f5ee6e-bfb1-4813-93aa-1af8b2b75bbd')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a2f5ee6e-bfb1-4813-93aa-1af8b2b75bbd button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "0                     dataset_name            sampleID subjectID bodysite  \\\n",
              "1   Zeller_fecal_colorectal_cancer  CCIS00146684ST-4-0    fr-726    stool   \n",
              "2   Zeller_fecal_colorectal_cancer  CCIS00281083ST-3-0    fr-060    stool   \n",
              "3   Zeller_fecal_colorectal_cancer  CCIS02124300ST-4-0    fr-568    stool   \n",
              "4   Zeller_fecal_colorectal_cancer  CCIS02379307ST-4-0    fr-828    stool   \n",
              "5   Zeller_fecal_colorectal_cancer  CCIS03473770ST-4-0    fr-192    stool   \n",
              "6   Zeller_fecal_colorectal_cancer  CCIS06260551ST-3-0    fr-200    stool   \n",
              "7   Zeller_fecal_colorectal_cancer  CCIS07539127ST-4-0    fr-460    stool   \n",
              "8   Zeller_fecal_colorectal_cancer  CCIS07648107ST-4-0    fr-053    stool   \n",
              "9   Zeller_fecal_colorectal_cancer  CCIS08668806ST-3-0    fr-214    stool   \n",
              "10  Zeller_fecal_colorectal_cancer  CCIS09568613ST-4-0    fr-400    stool   \n",
              "\n",
              "0         disease age  gender country sequencing_technology  pubmedid  ...  \\\n",
              "1               n  72  female  france              Illumina  25432777  ...   \n",
              "2               n  53    male  france              Illumina  25432777  ...   \n",
              "3               n  35    male  france              Illumina  25432777  ...   \n",
              "4          cancer  67    male  france              Illumina  25432777  ...   \n",
              "5               n  29    male  france              Illumina  25432777  ...   \n",
              "6          cancer  58    male  france              Illumina  25432777  ...   \n",
              "7               n  77  female  france              Illumina  25432777  ...   \n",
              "8               n  62  female  france              Illumina  25432777  ...   \n",
              "9   small_adenoma  63    male  france              Illumina  25432777  ...   \n",
              "10              n  67    male  france              Illumina  25432777  ...   \n",
              "\n",
              "0  k__Eukaryota|p__Ascomycota|c__Saccharomycetes|o__Saccharomycetales|f__Saccharomycetaceae|g__Eremothecium|s__Eremothecium_unclassified  \\\n",
              "1                                                   0                                                                                      \n",
              "2                                                   0                                                                                      \n",
              "3                                                   0                                                                                      \n",
              "4                                                   0                                                                                      \n",
              "5                                                   0                                                                                      \n",
              "6                                                   0                                                                                      \n",
              "7                                                   0                                                                                      \n",
              "8                                                   0                                                                                      \n",
              "9                                                   0                                                                                      \n",
              "10                                                  0                                                                                      \n",
              "\n",
              "0  k__Bacteria|p__Firmicutes|c__Bacilli|o__Lactobacillales|f__Lactobacillaceae|g__Lactobacillus|s__Lactobacillus_antri  \\\n",
              "1                                                   0                                                                    \n",
              "2                                                   0                                                                    \n",
              "3                                                   0                                                                    \n",
              "4                                                   0                                                                    \n",
              "5                                                   0                                                                    \n",
              "6                                                   0                                                                    \n",
              "7                                                   0                                                                    \n",
              "8                                                   0                                                                    \n",
              "9                                                   0                                                                    \n",
              "10                                                  0                                                                    \n",
              "\n",
              "0  k__Bacteria|p__Firmicutes|c__Bacilli|o__Bacillales|f__Bacillaceae|g__Lysinibacillus|s__Lysinibacillus_fusiformis  \\\n",
              "1                                                   0                                                                 \n",
              "2                                                   0                                                                 \n",
              "3                                                   0                                                                 \n",
              "4                                                   0                                                                 \n",
              "5                                                   0                                                                 \n",
              "6                                             0.31121                                                                 \n",
              "7                                                   0                                                                 \n",
              "8                                                   0                                                                 \n",
              "9                                                   0                                                                 \n",
              "10                                                  0                                                                 \n",
              "\n",
              "0  k__Archaea|p__Euryarchaeota|c__Methanobacteria|o__Methanobacteriales|f__Methanobacteriaceae|g__Methanobacterium|s__Methanobacterium_unclassified  \\\n",
              "1                                                   0                                                                                                 \n",
              "2                                                   0                                                                                                 \n",
              "3                                                   0                                                                                                 \n",
              "4                                                   0                                                                                                 \n",
              "5                                                   0                                                                                                 \n",
              "6                                                   0                                                                                                 \n",
              "7                                                   0                                                                                                 \n",
              "8                                                   0                                                                                                 \n",
              "9                                                   0                                                                                                 \n",
              "10                                                  0                                                                                                 \n",
              "\n",
              "0  k__Bacteria|p__Firmicutes|c__Bacilli|o__Bacillales|f__Bacillaceae|g__Lysinibacillus|s__Lysinibacillus_boronitolerans  \\\n",
              "1                                                   0                                                                     \n",
              "2                                                   0                                                                     \n",
              "3                                                   0                                                                     \n",
              "4                                                   0                                                                     \n",
              "5                                                   0                                                                     \n",
              "6                                             0.03562                                                                     \n",
              "7                                                   0                                                                     \n",
              "8                                                   0                                                                     \n",
              "9                                                   0                                                                     \n",
              "10                                                  0                                                                     \n",
              "\n",
              "0  k__Bacteria|p__Firmicutes|c__Bacilli|o__Lactobacillales|f__Enterococcaceae|g__Bavariicoccus|s__Bavariicoccus_seileri  \\\n",
              "1                                                   0                                                                     \n",
              "2                                                   0                                                                     \n",
              "3                                                   0                                                                     \n",
              "4                                                   0                                                                     \n",
              "5                                                   0                                                                     \n",
              "6                                                   0                                                                     \n",
              "7                                                   0                                                                     \n",
              "8                                                   0                                                                     \n",
              "9                                                   0                                                                     \n",
              "10                                                  0                                                                     \n",
              "\n",
              "0  k__Bacteria|p__Firmicutes|c__Bacilli|o__Lactobacillales|f__Enterococcaceae|g__Enterococcus|s__Enterococcus_gilvus  \\\n",
              "1                                                   0                                                                  \n",
              "2                                                   0                                                                  \n",
              "3                                                   0                                                                  \n",
              "4                                                   0                                                                  \n",
              "5                                             0.03756                                                                  \n",
              "6                                                   0                                                                  \n",
              "7                                                   0                                                                  \n",
              "8                                                   0                                                                  \n",
              "9                                                   0                                                                  \n",
              "10                                                  0                                                                  \n",
              "\n",
              "0  k__Bacteria|p__Firmicutes|c__Bacilli|o__Lactobacillales|f__Lactobacillaceae|g__Lactobacillus|s__Lactobacillus_otakiensis  \\\n",
              "1                                                   0                                                                         \n",
              "2                                                   0                                                                         \n",
              "3                                                   0                                                                         \n",
              "4                                                   0                                                                         \n",
              "5                                                   0                                                                         \n",
              "6                                                   0                                                                         \n",
              "7                                                   0                                                                         \n",
              "8                                                   0                                                                         \n",
              "9                                                   0                                                                         \n",
              "10                                                  0                                                                         \n",
              "\n",
              "0  k__Bacteria|p__Firmicutes|c__Clostridia|o__Clostridiales|f__Peptococcaceae|g__Desulfotomaculum|s__Desulfotomaculum_ruminis  \\\n",
              "1                                                   0                                                                           \n",
              "2                                                   0                                                                           \n",
              "3                                                   0                                                                           \n",
              "4                                                   0                                                                           \n",
              "5                                                   0                                                                           \n",
              "6                                                   0                                                                           \n",
              "7                                                   0                                                                           \n",
              "8                                                   0                                                                           \n",
              "9                                                   0                                                                           \n",
              "10                                                  0                                                                           \n",
              "\n",
              "0  k__Bacteria|p__Firmicutes|c__Negativicutes|o__Selenomonadales|f__Veillonellaceae|g__Megasphaera|s__Megasphaera_sp_BV3C16_1  \n",
              "1                                                   0                                                                          \n",
              "2                                                   0                                                                          \n",
              "3                                                   0                                                                          \n",
              "4                                                   0                                                                          \n",
              "5                                                   0                                                                          \n",
              "6                                                   0                                                                          \n",
              "7                                                   0                                                                          \n",
              "8                                                   0                                                                          \n",
              "9                                                   0                                                                          \n",
              "10                                                  0                                                                          \n",
              "\n",
              "[10 rows x 714 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Extract a sample data of colorectal cancer of microbiome abundance profile\n",
        "sample = pd.read_csv(os.path.join(raw_data_dir, \"abundance\", \"abundance_Colorectal.txt\"), sep='\\t', index_col=0, header=None)\n",
        "sample = sample.T\n",
        "sample.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EjqDkrWm0lc",
        "outputId": "c794afa1-010d-4157-c5d0-935310bb2f24"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "503"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Count the features excluding patients information\n",
        "len([ _ for _ in sample.columns if '|' in _ ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i93VorpFlPnl",
        "outputId": "38e4269f-51ee-4de2-c5f0-3636a9c34837"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(121, 714)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Calculate the dimensions of the sameple dataset\n",
        "sample.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        },
        "id": "PPFi9QXolIoa",
        "outputId": "b7b78cd6-a15d-4088-9c05-922bc19452b7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"sample\",\n  \"rows\": 714,\n  \"fields\": [\n    {\n      \"column\": \"count\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"121\",\n        \"max\": \"121\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"121\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"unique\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 1,\n        \"max\": 121,\n        \"num_unique_values\": 103,\n        \"samples\": [\n          23\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"top\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 21,\n        \"samples\": [\n          \"Zeller_fecal_colorectal_cancer\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"freq\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"1\",\n        \"max\": \"121\",\n        \"num_unique_values\": 105,\n        \"samples\": [\n          \"77\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-46d06a0a-dbd7-4798-bd74-3b25b82ee5fd\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>unique</th>\n",
              "      <th>top</th>\n",
              "      <th>freq</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>dataset_name</th>\n",
              "      <td>121</td>\n",
              "      <td>1</td>\n",
              "      <td>Zeller_fecal_colorectal_cancer</td>\n",
              "      <td>121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sampleID</th>\n",
              "      <td>121</td>\n",
              "      <td>121</td>\n",
              "      <td>CCIS00146684ST-4-0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>subjectID</th>\n",
              "      <td>121</td>\n",
              "      <td>121</td>\n",
              "      <td>fr-726</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodysite</th>\n",
              "      <td>121</td>\n",
              "      <td>1</td>\n",
              "      <td>stool</td>\n",
              "      <td>121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>disease</th>\n",
              "      <td>121</td>\n",
              "      <td>3</td>\n",
              "      <td>cancer</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>k__Bacteria|p__Firmicutes|c__Bacilli|o__Lactobacillales|f__Enterococcaceae|g__Bavariicoccus|s__Bavariicoccus_seileri</th>\n",
              "      <td>121</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>k__Bacteria|p__Firmicutes|c__Bacilli|o__Lactobacillales|f__Enterococcaceae|g__Enterococcus|s__Enterococcus_gilvus</th>\n",
              "      <td>121</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>k__Bacteria|p__Firmicutes|c__Bacilli|o__Lactobacillales|f__Lactobacillaceae|g__Lactobacillus|s__Lactobacillus_otakiensis</th>\n",
              "      <td>121</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>k__Bacteria|p__Firmicutes|c__Clostridia|o__Clostridiales|f__Peptococcaceae|g__Desulfotomaculum|s__Desulfotomaculum_ruminis</th>\n",
              "      <td>121</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>k__Bacteria|p__Firmicutes|c__Negativicutes|o__Selenomonadales|f__Veillonellaceae|g__Megasphaera|s__Megasphaera_sp_BV3C16_1</th>\n",
              "      <td>121</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>120</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>714 rows × 4 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-46d06a0a-dbd7-4798-bd74-3b25b82ee5fd')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-46d06a0a-dbd7-4798-bd74-3b25b82ee5fd button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-46d06a0a-dbd7-4798-bd74-3b25b82ee5fd');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-74cf3273-b172-4a98-876a-526db48ff33a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-74cf3273-b172-4a98-876a-526db48ff33a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-74cf3273-b172-4a98-876a-526db48ff33a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                   count unique  \\\n",
              "0                                                                 \n",
              "dataset_name                                         121      1   \n",
              "sampleID                                             121    121   \n",
              "subjectID                                            121    121   \n",
              "bodysite                                             121      1   \n",
              "disease                                              121      3   \n",
              "...                                                  ...    ...   \n",
              "k__Bacteria|p__Firmicutes|c__Bacilli|o__Lactoba...   121      2   \n",
              "k__Bacteria|p__Firmicutes|c__Bacilli|o__Lactoba...   121      5   \n",
              "k__Bacteria|p__Firmicutes|c__Bacilli|o__Lactoba...   121      2   \n",
              "k__Bacteria|p__Firmicutes|c__Clostridia|o__Clos...   121      2   \n",
              "k__Bacteria|p__Firmicutes|c__Negativicutes|o__S...   121      2   \n",
              "\n",
              "                                                                               top  \\\n",
              "0                                                                                    \n",
              "dataset_name                                        Zeller_fecal_colorectal_cancer   \n",
              "sampleID                                                        CCIS00146684ST-4-0   \n",
              "subjectID                                                                   fr-726   \n",
              "bodysite                                                                     stool   \n",
              "disease                                                                     cancer   \n",
              "...                                                                            ...   \n",
              "k__Bacteria|p__Firmicutes|c__Bacilli|o__Lactoba...                               0   \n",
              "k__Bacteria|p__Firmicutes|c__Bacilli|o__Lactoba...                               0   \n",
              "k__Bacteria|p__Firmicutes|c__Bacilli|o__Lactoba...                               0   \n",
              "k__Bacteria|p__Firmicutes|c__Clostridia|o__Clos...                               0   \n",
              "k__Bacteria|p__Firmicutes|c__Negativicutes|o__S...                               0   \n",
              "\n",
              "                                                   freq  \n",
              "0                                                        \n",
              "dataset_name                                        121  \n",
              "sampleID                                              1  \n",
              "subjectID                                             1  \n",
              "bodysite                                            121  \n",
              "disease                                              48  \n",
              "...                                                 ...  \n",
              "k__Bacteria|p__Firmicutes|c__Bacilli|o__Lactoba...  120  \n",
              "k__Bacteria|p__Firmicutes|c__Bacilli|o__Lactoba...  117  \n",
              "k__Bacteria|p__Firmicutes|c__Bacilli|o__Lactoba...  120  \n",
              "k__Bacteria|p__Firmicutes|c__Clostridia|o__Clos...  120  \n",
              "k__Bacteria|p__Firmicutes|c__Negativicutes|o__S...  120  \n",
              "\n",
              "[714 rows x 4 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Have a generate descrption of the sample dataset\n",
        "sample.describe().T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTkZz-HrofL1"
      },
      "source": [
        "### Data Process\n",
        "\n",
        "According to the original paper, the data provided in the codebase have been preprocessed and cleaned properly. We only need to extract the labels columns with feature index indentifier to get `X`. Also, we get `Y` from the `disease` column. This step is implemented in the `loadData` function of class `DeepMiccrobiome` in the Model session below.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk5NZf3ToGy6"
      },
      "source": [
        "### Data Use\n",
        "\n",
        "Given the running time of dimensionality reduction is long, we choose to only use the first 10 features of each dataset in order to control the total latency in this draft project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3muyDPFPbozY"
      },
      "source": [
        "##   Model\n",
        "The model includes the model definitation which usually is a class, model training, and other necessary parts.\n",
        "  * Model architecture: layer number/size/type, activation function, etc\n",
        "  * Training objectives: loss function, optimizer, weight of each loss term, etc\n",
        "  * Others: whether the model is pretrained, Monte Carlo simulation for uncertainty analysis, etc\n",
        "  * The code of model should have classes of the model, functions of model training, model validation, etc.\n",
        "  * If your model training is done outside of this notebook, please upload the trained model here and develop a function to load and test it.\n",
        "\n",
        "### Model Architecture\n",
        "\n",
        "An autoencoder represents a type of neural network designed for the purpose of reconstructing its input data, denoted as $x$. In its fundamental structure, it comprises an encoder function, denoted as $f_\\phi(⋅)$, and a decoder function, denoted as $f_\\theta′(⋅)$, with $\\phi$ and $\\theta$ serving as the parameters associated with the encoder and decoder functions, respectively.\n",
        "The training objective of an autoencoder is to minimize the disparity between the original input $x$ and its reconstructed counterpart $x′$. This discrepancy, typically quantified using a reconstruction loss metric such as squared error, can be mathematically expressed as $L(x, x')=||x-x'||^2=||x-f_\\theta'(f_\\phi(x))||^2$.\n",
        "\n",
        "In this project, we focus on the utilization of a trained autoencoder to obtain a lower-dimensional latent representation $z = f_\\phi(x)$ of the input. There are four autoencoders that we incorporate as below.\n",
        "\n",
        "1. Shallow Autoencoder (SAE): a fully connected encoder connecting the input layer to the latent layer, and a decoder producing the reconstructed input $x′$ by combining the outputs of the latent layer using weighted sums, with both the latent and output layers utilizing a linear activation function.\n",
        "\n",
        "2. Deep Autoencoder (DAE): enhanced SAE model by inserting hidden layers with Rectified Linear Unit (ReLu) activation function and Glorot uniform initializer between the input and latent layers, maintaining an equal number of hidden layers (either one or two layers) in both the encoder and decoder sections.\n",
        "\n",
        "3. Variational autoencoder (VAE): it learns probabilistic representations $z$ by approximating the true posterior distribution with $q_\\phi(z|x)$ assuming a Gaussian distribution. The encoder encodes the means and variances of the Gaussian distribution, allowing sampling of latent representation $z$. This sampled representation is then fed into the decoder network to generate the reconstructed input $x′ \\sim g_\\theta(x|z)$.\n",
        "\n",
        "4. convolutional autoencoder (CAE): equipped with convolutional layers where each unit is connected locally to the previous layer. The layers consist of multiple filters with weights for convolution operations. We employed ReLu activation, Glorot uniform initializer, and avoided pooling layers to prevent excessive generalization. The $n$-dimensional input vector was reshaped into a squared image of size $d \\times d \\times 1$, where $d = ⌊ \\sqrt{n} ⌋ + 1$.\n",
        "\n",
        "After getting the low-dimentional representations of profiles, we build classification models.\n",
        "\n",
        "1. Support Vector Machine (SVM): a grid search using SVM is conducted to explore the hyper-parameter space. We considered both radial basis function (RBF) and linear kernels, adjusting penalty parameter C and kernel coefficient gamma for RBF.\n",
        "\n",
        "2. Random Forest (RF): We examine two criteria, Gini impurity and information gain, for selecting features to split a node in a decision tree. The maximum number of features considered for the best split at each node was determined using the square root of the sample size and the logarithm to base 2 of the sample size.\n",
        "\n",
        "3. Multi-Layer Perceptron (MLP): ReLu activations were used for the hidden layers, while the output layer employed sigmoid activation with a single unit. The number of units in the hidden layers was set to half of the preceding layer, excluding the first hidden layer.\n",
        "\n",
        "The overall workflow is demonstrated in the Figure 1 below.\n",
        "![](assets/Model_Figure1.png)\n",
        "\n",
        "### Training Objectives\n",
        "- Split each dataset into a training set, validation set, and test set (64% training, 16% validation, and 20% test).\n",
        "- Exclude the test set from model training.\n",
        "- Implement early-stopping strategy: train models on the training set, compute reconstruction loss for the validation set after each epoch, stop training if no improvement in validation loss is observed for 20 epochs.\n",
        "- Select the model with the lowest validation loss as the best model.\n",
        "Utilize mean squared error as the reconstruction loss metric.\n",
        "- Apply adaptive moment estimation (Adam) optimizer with default parameters (learning rate: 0.001, epsilon: 1e-07) as specified in the original paper.\n",
        "- Utilize the encoder part of the best model to generate low-dimensional representations of microbiome data for subsequent disease prediction tasks.\n",
        "\n",
        "\n",
        "### Evaluations\n",
        "\n",
        "- Conduct 5-fold cross-validation on the reduced training set. This involves dividing the training set into five subsets, using four subsets for training and one for validation in each fold. This is to vary hyper-parameters and explore different combinations to find the best performing configuration.\n",
        "- Evaluate the performance of the models using the area under the receiver operating characteristics curve (AUC). This metric assesses the model's ability to distinguish between different classes and is commonly used for classification tasks.\n",
        "- Train a final classification model using the entire training set and the best hyper-parameter combination identified during cross-validation. This model aims to achieve optimal performance based on the selected configuration. Then test the final classification model on the separate test set, which was not used during training. This evaluation provides an unbiased assessment of the model's performance on unseen data.\n",
        "- Repeat the entire procedure five times, each time using a different random partition seed to create new training, validation, and test sets. This helps account for potential variations in performance due to the specific data splits. Average the resulting AUC scores obtained from the five repetitions. This average serves as a summary metric to compare the performance of different models or approaches. It provides a more robust assessment by considering multiple iterations of the evaluation process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RR6vBH8DsFqD"
      },
      "source": [
        "### Hyperparameters Config\n",
        "\n",
        "The original implementation use command line arguments to facilitate the execution of experiments with varying configurations. In order to enhance usability within a Colab notebook environment, we modified this approach by introducing a configuration object, which offers a more convenient and intuitive means of manipulating settings. The structures of configs is as below.\n",
        "\n",
        "```\n",
        ".\n",
        "├── ...\n",
        "├── data                                      # Data folder\n",
        "├── experiment_configs                        # Config folder\n",
        "│       ├── vae_config.json                   # Config uses Variational Autoencoder (VAE)\n",
        "│       ├── cae_config.json                   # Config uses Convolutional Autoencoder (CAE):\n",
        "│       ├── ae_config.json                    # Config uses Shallow Autoencoder (SAE) or Deep Autoencoder (DAE)\n",
        "│       ├── default_experiment_config.json    # Baseline configs\n",
        "│       ├── test_experiment_config_1.json     # Test config of no autoencoder\n",
        "└── ...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "BHu3D1U5sF03"
      },
      "outputs": [],
      "source": [
        "# experiment config dir\n",
        "experiments_config_dir = '/content/drive/My Drive/Colab Notebooks/experiment_configs'\n",
        "\n",
        "# set up config class for loading json experiment configs\n",
        "class DeepMicro_Config(object):\n",
        "  def __init__(self, config_name=None, config_dict=None):\n",
        "    if config_dict:\n",
        "      self.load_from_dict(config_dict)\n",
        "    elif config_name:\n",
        "      self.load_from_file(experiments_config_dir + \"/\" + config_name + \".json\")\n",
        "\n",
        "  def load_from_dict(self, dictionary):\n",
        "    for key, value in dictionary.items():\n",
        "      if isinstance(value, dict):\n",
        "        value = DeepMicro_Config(config_dict=value)\n",
        "      self.__dict__[key] = value\n",
        "\n",
        "  def load_from_file(self, config_path):\n",
        "    with open(config_path, 'r') as f:\n",
        "      config_dict = json.load(f)\n",
        "    self.load_from_dict(config_dict)\n",
        "\n",
        "  def __getattr__(self, attr):\n",
        "    return self.__dict__.get(attr, None)\n",
        "\n",
        "# dict for data_type in config\n",
        "dtypeDict = {\"float16\": np.float16, \"float32\": np.float32, \"float64\": np.float64}\n",
        "\n",
        "# set labels for diseases and controls\n",
        "label_dict = {\n",
        "  # Controls\n",
        "  'n': 0,\n",
        "  # Chirrhosis\n",
        "  'cirrhosis': 1,\n",
        "  # Colorectal Cancer\n",
        "  'cancer': 1, 'small_adenoma': 0,\n",
        "  # IBD\n",
        "  'ibd_ulcerative_colitis': 1, 'ibd_crohn_disease': 1,\n",
        "  # T2D and WT2D\n",
        "  't2d': 1,\n",
        "  # Obesity\n",
        "  'leaness': 0, 'obesity': 1,\n",
        "}\n",
        "\n",
        "# hyper-parameter grids for classifiers\n",
        "# TODO: set n_estimator range(100, 1001, 200), here is only for draft\n",
        "# TODO: set min_samples_leaf range(1,6), here is only for draft\n",
        "rf_hyper_parameters = [{'n_estimators': [s for s in range(500, 1000, 500)],\n",
        "                        'max_features': ['sqrt', 'log2'],\n",
        "                        'min_samples_leaf': [2], # [1, 2, 3, 4, 5],\n",
        "                        'criterion': ['gini', 'entropy']\n",
        "                        }, ]\n",
        "\n",
        "#svm_hyper_parameters_pasolli = [{'C': [2 ** s for s in range(-5, 16, 2)], 'kernel': ['linear']},\n",
        "#                        {'C': [2 ** s for s in range(-5, 16, 2)], 'gamma': [2 ** s for s in range(3, -15, -2)],\n",
        "#                         'kernel': ['rbf']}]\n",
        "\n",
        "# TODO: set C range(-5, 6, 2), here is only for draft\n",
        "# TODO: set gamma range(3, -15, -2), here is only for draft\n",
        "svm_hyper_parameters = [\n",
        "    {\n",
        "        'C': [2 ** s for s in [-2]],\n",
        "        'kernel': ['linear']\n",
        "        },\n",
        "    {\n",
        "        'C': [2 ** s for s in [-2]],\n",
        "        'gamma': [2 ** s for s in range(1, -4, -2)],\n",
        "        'kernel': ['rbf']\n",
        "        }\n",
        "    ]\n",
        "# TODO: set numHiddenLayers [1, 2, 3], here is only for draft\n",
        "# TODO: set epoch larger in final project, 30 is only for draft\n",
        "# TODO: set numUnits larger in final project, 10 is only for draft\n",
        "mlp_hyper_parameters = [{'numHiddenLayers': [2],\n",
        "                         'epochs': [30], # [30, 50, 100, 200, 300],\n",
        "                         'numUnits': [10], # [10, 30, 50, 100],\n",
        "                         'dropout_rate': [0.1, 0.3],\n",
        "                         },]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3XEF8AmtwSI"
      },
      "source": [
        "### Autoencoders Definitions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pWkNlD2ORZoG"
      },
      "outputs": [],
      "source": [
        "# Autoencoder\n",
        "def autoencoder(dims, act='relu', init='glorot_uniform', latent_act = False, output_act = False):\n",
        "    \"\"\"\n",
        "        Fully connected auto-encoder model, symmetric.\n",
        "        Arguments:\n",
        "            dims: list of number of units in each layer of encoder. dims[0] is input dim, dims[-1] is units in hidden layer.\n",
        "                The decoder is symmetric with encoder. So number of layers of the auto-encoder is 2*len(dims)-1\n",
        "            act: activation, not applied to Input, Hidden and Output layers\n",
        "        return:\n",
        "            (ae_model, encoder_model), Model of autoencoder and model of encoder\n",
        "        \"\"\"\n",
        "\n",
        "    # whether put activation function in latent layer\n",
        "    if latent_act:\n",
        "        l_act = act\n",
        "    else:\n",
        "        l_act = None\n",
        "\n",
        "    if output_act:\n",
        "        o_act = 'sigmoid'\n",
        "    else:\n",
        "        o_act = None\n",
        "\n",
        "    # The number of internal layers: layers between input and latent layer\n",
        "    n_internal_layers = len(dims) - 2\n",
        "\n",
        "    # input\n",
        "    x = Input(shape=(dims[0],), name='input')\n",
        "    h = x\n",
        "\n",
        "    # internal layers in encoder\n",
        "    for i in range(n_internal_layers):\n",
        "        h = Dense(dims[i + 1], activation=act, kernel_initializer=init, name='encoder_%d' % i)(h)\n",
        "\n",
        "    # bottle neck layer, features are extracted from here\n",
        "    h = Dense(dims[-1], activation=l_act, kernel_initializer=init, name='encoder_%d_bottle-neck' % (n_internal_layers))(h)\n",
        "\n",
        "    y = h\n",
        "\n",
        "    # internal layers in decoder\n",
        "    for i in range(n_internal_layers, 0, -1):\n",
        "        y = Dense(dims[i], activation=act, kernel_initializer=init, name='decoder_%d' % i)(y)\n",
        "\n",
        "    # output\n",
        "    y = Dense(dims[0], activation=o_act, kernel_initializer=init, name='decoder_0')(y)\n",
        "\n",
        "    return Model(inputs=x, outputs=y, name='AE'), Model(inputs=x, outputs=h, name='encoder')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "79KCL-48RxbA"
      },
      "outputs": [],
      "source": [
        "# Convolutional autoencoder\n",
        "def conv_autoencoder(dims, act='relu', init='glorot_uniform', latent_act = False, output_act = False, rf_rate = 0.1, st_rate = 0.25):\n",
        "    # whether put activation function in latent layer\n",
        "    if latent_act:\n",
        "        l_act = act\n",
        "    else:\n",
        "        l_act = None\n",
        "\n",
        "    if output_act:\n",
        "        o_act = 'sigmoid'\n",
        "    else:\n",
        "        o_act = None\n",
        "\n",
        "    # receptive field and stride size\n",
        "    rf_size = init_rf_size = int(dims[0][0] * rf_rate)\n",
        "    stride_size = init_stride_size = int(rf_size * st_rate) if int(rf_size * st_rate) > 0 else 1\n",
        "    print(\"receptive field (kernel) size: %d\" % rf_size)\n",
        "    print(\"stride size: %d\" % stride_size)\n",
        "\n",
        "    # The number of internal layers: layers between input and latent layer\n",
        "    n_internal_layers = len(dims) - 1\n",
        "\n",
        "    if n_internal_layers < 1:\n",
        "        print(\"The number of internal layers for CAE should be greater than or equal to 1\")\n",
        "        exit()\n",
        "\n",
        "    # input\n",
        "    x = Input(shape=dims[0], name='input')\n",
        "    h = x\n",
        "\n",
        "    rf_size_list = []\n",
        "    stride_size_list = []\n",
        "    # internal layers in encoder\n",
        "    for i in range(n_internal_layers):\n",
        "        print(\"rf_size: %d, st_size: %d\" % (rf_size, stride_size))\n",
        "        h = Conv2D(\n",
        "            dims[max(i + 1, len(dims)-1)], (rf_size,rf_size),\n",
        "            strides=(stride_size, stride_size), activation=act,\n",
        "            padding='same', kernel_initializer=init,\n",
        "            name='encoder_conv_%d' % i)(h)\n",
        "        #h = MaxPool2D((2,2), padding='same')(h)\n",
        "        rf_size = int(K.int_shape(h)[1] * rf_rate)\n",
        "        stride_size = int(rf_size /2.) if int(rf_size /2.) > 0 else 1\n",
        "        rf_size_list.append(rf_size)\n",
        "        stride_size_list.append(stride_size)\n",
        "\n",
        "    reshapeDim = K.int_shape(h)[1:]\n",
        "\n",
        "    # bottle neck layer, features are extracted from h\n",
        "    h = Flatten()(h)\n",
        "\n",
        "    y = h\n",
        "\n",
        "    y = Reshape(reshapeDim)(y)\n",
        "\n",
        "    print(rf_size_list)\n",
        "    print(stride_size_list)\n",
        "\n",
        "    # internal layers in decoder\n",
        "    for i in range(n_internal_layers - 1, 0, -1):\n",
        "        y = Conv2DTranspose(dims[i], (rf_size_list[i-1],rf_size_list[i-1]), strides=(stride_size_list[i-1], stride_size_list[i-1]), activation=act, padding='same', kernel_initializer=init, name='decoder_conv_%d' % i)(y)\n",
        "        #y = UpSampling2D((2,2))(y)\n",
        "\n",
        "    y = Conv2DTranspose(1, (init_rf_size, init_rf_size), strides=(init_stride_size, init_stride_size), activation=o_act, kernel_initializer=init, padding='same', name='decoder_1')(y)\n",
        "\n",
        "    # output cropping\n",
        "    if K.int_shape(x)[1] != K.int_shape(y)[1]:\n",
        "        cropping_size = K.int_shape(y)[1] - K.int_shape(x)[1]\n",
        "        y = Cropping2D(cropping=((cropping_size, 0), (cropping_size, 0)), data_format=None)(y)\n",
        "\n",
        "    #print(\"dims[0]: %s\" % str(dims[0]))\n",
        "\n",
        "    # output\n",
        "    # y = Conv2D(1, (rf_size, rf_size), activation=o_act, kernel_initializer=init, padding='same', name='decoder_1')(y)\n",
        "    #\n",
        "    # outputDim = reshapeDim * (2 ** n_internal_layers)\n",
        "    # if outputDim != dims[0][0]:\n",
        "    #     cropping_size = outputDim - dims[0][0]\n",
        "    #     #print(outputDim, dims[0][0], cropping_size)\n",
        "    #     y = Cropping2D(cropping=((cropping_size, 0), (cropping_size, 0)), data_format=None)(y)\n",
        "\n",
        "\n",
        "    return Model(inputs=x, outputs=y, name='CAE'), Model(inputs=x, outputs=h, name='encoder')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Gi-t3nmPRxhZ"
      },
      "outputs": [],
      "source": [
        "# reparameterization trick used in VAE\n",
        "# instead of sampling from Q(z|X), sample epsilon = N(0,I)\n",
        "# z = z_mean + sqrt(var) * epsilon\n",
        "def sampling(args):\n",
        "    z_mean, z_sigma = args\n",
        "    #z_mean, z_log_var = args\n",
        "    batch = K.shape(z_mean)[0]\n",
        "    dim = K.int_shape(z_mean)[1]\n",
        "    # by default, random_normal has mean = 0 and std = 1.0\n",
        "    epsilon = K.random_normal(shape=(batch, dim))\n",
        "    return z_mean + z_sigma * epsilon\n",
        "    #return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "# Variational Autoencoder\n",
        "def variational_AE(dims, act='relu', init='glorot_uniform', output_act = False, recon_loss = 'mse', beta=1):\n",
        "\n",
        "    if output_act:\n",
        "        o_act = 'sigmoid'\n",
        "    else:\n",
        "        o_act = None\n",
        "\n",
        "    # The number of internal layers: layers between input and latent layer\n",
        "    n_internal_layers = len(dims) - 2\n",
        "\n",
        "    ## build encoder model\n",
        "    inputs = Input(shape=(dims[0],), name='input')\n",
        "\n",
        "    h = inputs\n",
        "\n",
        "    # internal layers in encoder\n",
        "    for i in range(n_internal_layers):\n",
        "        h = Dense(dims[i + 1], activation=act, kernel_initializer=init, name='encoder_%d' % i)(h)\n",
        "\n",
        "    # latent layer\n",
        "    z_mean = Dense(dims[-1], name='z_mean')(h)\n",
        "    z_sigma = Dense(dims[-1], name='z_sigma')(h)\n",
        "\n",
        "    # z_log_var = Dense(dims[-1], name='z_log_var')(h)\n",
        "\n",
        "    # use reparameterization trick to push the sampling out as input\n",
        "    # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
        "    z = Lambda(sampling, output_shape=(dims[-1],), name='z')([z_mean, z_sigma])\n",
        "\n",
        "    # instantiate encoder model\n",
        "    encoder = Model(inputs, [z_mean, z_sigma, z], name='encoder')\n",
        "\n",
        "    ## build decoder model\n",
        "    latent_inputs = Input(shape=(dims[-1],), name='z_sampling')\n",
        "\n",
        "    y = latent_inputs\n",
        "\n",
        "    # internal layers in decoder\n",
        "    for i in range(n_internal_layers, 0, -1):\n",
        "        y = Dense(dims[i], activation=act, kernel_initializer=init, name='decoder_%d' % i)(y)\n",
        "\n",
        "    outputs = Dense(dims[0], kernel_initializer=init, activation=o_act)(y)\n",
        "\n",
        "    # instantiate decoder model\n",
        "    decoder = Model(latent_inputs, outputs, name='decoder')\n",
        "\n",
        "    # instantiate VAE model\n",
        "    outputs = decoder(encoder(inputs)[2])\n",
        "    vae = Model(inputs, outputs, name='vae_mlp')\n",
        "\n",
        "    # TODO: manually initiate the metrics list\n",
        "    vae.metrics_tensors = []\n",
        "\n",
        "\n",
        "    ## loss function\n",
        "    if recon_loss == 'mse':\n",
        "        reconstruction_loss = mse(inputs, outputs)\n",
        "    else:\n",
        "        reconstruction_loss = binary_crossentropy(inputs,\n",
        "                                                  outputs)\n",
        "\n",
        "    reconstruction_loss *= dims[0]\n",
        "\n",
        "    kl_loss = 1 + K.log(1e-8 + K.square(z_sigma)) - K.square(z_mean) - K.square(z_sigma)\n",
        "    #kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
        "    kl_loss = K.sum(kl_loss, axis=-1)\n",
        "    kl_loss *= -0.5\n",
        "\n",
        "    vae_loss = K.mean(reconstruction_loss + (beta * kl_loss))\n",
        "    vae.add_loss(vae_loss)\n",
        "\n",
        "    vae.compile(optimizer='adam', )\n",
        "\n",
        "    vae.metrics_tensors.append(K.mean(reconstruction_loss))\n",
        "    vae.metrics_names.append(\"recon_loss\")\n",
        "    vae.metrics_tensors.append(K.mean(beta * kl_loss))\n",
        "    vae.metrics_names.append(\"kl_loss\")\n",
        "\n",
        "    return vae, encoder, decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1LI2I3puU3g"
      },
      "source": [
        "### Multi-Layer Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yMryKTfuVulb"
      },
      "outputs": [],
      "source": [
        "# create MLP model\n",
        "def mlp_model(input_dim, numHiddenLayers=3, numUnits=64, dropout_rate=0.5):\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    #Check number of hidden layers\n",
        "    if numHiddenLayers >= 1:\n",
        "        # First Hidden layer\n",
        "        model.add(Dense(numUnits, input_dim=input_dim, activation='relu'))\n",
        "        model.add(Dropout(dropout_rate))\n",
        "\n",
        "        # Second to the last hidden layers\n",
        "        for i in range(numHiddenLayers - 1):\n",
        "            numUnits = numUnits // 2\n",
        "            model.add(Dense(numUnits, activation='relu'))\n",
        "            model.add(Dropout(dropout_rate))\n",
        "\n",
        "        # output layer\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    else:\n",
        "        # output layer\n",
        "        model.add(Dense(1, input_dim=input_dim, activation='sigmoid'))\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', )#metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS3jIzt9uj9B"
      },
      "source": [
        "### Train Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5WDva6WosGE6"
      },
      "outputs": [],
      "source": [
        "class DeepMicrobiome(object):\n",
        "  # TODO:  THIS WHOLE CLASS\n",
        "  # This is where the bulk of the logic for the project goes\n",
        "  def __init__(self, data, seed, data_dir):\n",
        "    self.t_start = time.time()\n",
        "    self.filename = str(data)\n",
        "    self.data = self.filename.split('.')[0]\n",
        "    self.seed = seed\n",
        "    self.data_dir = data_dir\n",
        "    self.prefix = ''\n",
        "    self.representation_only = False\n",
        "\n",
        "  def loadData(self, feature_string, label_string, label_dict, dtype=None):\n",
        "    # read file\n",
        "    filename = self.data_dir + self.filename\n",
        "    if os.path.isfile(filename):\n",
        "      raw = pd.read_csv(filename, sep='\\t', index_col=0, header=None)\n",
        "    else:\n",
        "      print(\"FileNotFoundError: File {} does not exist\".format(filename))\n",
        "      exit()\n",
        "\n",
        "    #DEBUG\n",
        "    print(\"loaded data from \" + str(filename))\n",
        "\n",
        "    # TODO: draft project only considers first 10 features\n",
        "\n",
        "    # select rows having feature index identifier string\n",
        "    X = raw.loc[raw.index.str.contains(feature_string, regex=False)].T\n",
        "    X = X.iloc[:, :10]\n",
        "\n",
        "    # get class labels\n",
        "    Y = raw.loc[label_string] #'disease'\n",
        "    Y = Y.replace(label_dict)\n",
        "\n",
        "    # train and test split\n",
        "    self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X.values.astype(dtype), Y.values.astype('int'), test_size=0.2, random_state=self.seed, stratify=Y.values)\n",
        "    self.printDataShapes()\n",
        "\n",
        "  def loadCustomData(self, dtype=None):\n",
        "    # read file\n",
        "    filename = self.data_dir + \"data/\" + self.filename\n",
        "    if os.path.isfile(filename):\n",
        "      raw = pd.read_csv(filename, sep=',', index_col=False, header=None)\n",
        "    else:\n",
        "      print(\"FileNotFoundError: File {} does not exist\".format(filename))\n",
        "      exit()\n",
        "\n",
        "    # load data\n",
        "    self.X_train = raw.values.astype(dtype)\n",
        "\n",
        "    # put nothing or zeros for y_train, y_test, and X_test\n",
        "    self.y_train = np.zeros(shape=(self.X_train.shape[0])).astype(dtype)\n",
        "    self.X_test = np.zeros(shape=(1,self.X_train.shape[1])).astype(dtype)\n",
        "    self.y_test = np.zeros(shape=(1,)).astype(dtype)\n",
        "    self.printDataShapes(train_only=True)\n",
        "\n",
        "  def loadCustomDataWithLabels(self, label_data, dtype=None):\n",
        "    # read file\n",
        "    filename = self.data_dir + \"data/\" + self.filename\n",
        "    label_filename = self.data_dir + \"data/\" + label_data\n",
        "    if os.path.isfile(filename) and os.path.isfile(label_filename):\n",
        "      raw = pd.read_csv(filename, sep=',', index_col=False, header=None)\n",
        "      label = pd.read_csv(label_filename, sep=',', index_col=False, header=None)\n",
        "    else:\n",
        "      if not os.path.isfile(filename):\n",
        "        print(\"FileNotFoundError: File {} does not exist\".format(filename))\n",
        "      if not os.path.isfile(label_filename):\n",
        "        print(\"FileNotFoundError: File {} does not exist\".format(label_filename))\n",
        "      exit()\n",
        "\n",
        "    # label data validity check\n",
        "    if not label.values.shape[1] > 1:\n",
        "      label_flatten = label.values.reshape((label.values.shape[0]))\n",
        "    else:\n",
        "      print('FileSpecificationError: The label file contains more than 1 column.')\n",
        "      exit()\n",
        "\n",
        "    # train and test split\n",
        "    self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(raw.values.astype(dtype),\n",
        "                                                                            label_flatten.astype('int'), test_size=0.2,\n",
        "                                                                            random_state=self.seed,\n",
        "                                                                            stratify=label_flatten)\n",
        "    self.printDataShapes()\n",
        "\n",
        "  # Principal Component Analysis\n",
        "  def pca(self, ratio=0.99):\n",
        "    # manipulating an experiment identifier in the output file\n",
        "    self.prefix = self.prefix + 'PCA_'\n",
        "\n",
        "    # PCA\n",
        "    pca = PCA()\n",
        "    pca.fit(self.X_train)\n",
        "    n_comp = 0\n",
        "    ratio_sum = 0.0\n",
        "\n",
        "    for comp in pca.explained_variance_ratio_:\n",
        "      ratio_sum += comp\n",
        "      n_comp += 1\n",
        "      if ratio_sum >= ratio:  # Selecting components explaining 99% of variance\n",
        "        break\n",
        "\n",
        "    pca = PCA(n_components=n_comp)\n",
        "    pca.fit(self.X_train)\n",
        "\n",
        "    X_train = pca.transform(self.X_train)\n",
        "    X_test = pca.transform(self.X_test)\n",
        "\n",
        "    # applying the eigenvectors to the whole training and the test set.\n",
        "    self.X_train = X_train\n",
        "    self.X_test = X_test\n",
        "    self.printDataShapes()\n",
        "\n",
        "  # Gausian Random Projection\n",
        "  def rp(self):\n",
        "    # manipulating an experiment identifier in the output file\n",
        "    self.prefix = self.prefix + 'RandP_'\n",
        "    # GRP\n",
        "    rf = GaussianRandomProjection(eps=0.5)\n",
        "    rf.fit(self.X_train)\n",
        "\n",
        "    # applying GRP to the whole training and the test set.\n",
        "    self.X_train = rf.transform(self.X_train)\n",
        "    self.X_test = rf.transform(self.X_test)\n",
        "    self.printDataShapes()\n",
        "\n",
        "  # Shallow Autoencoder & Deep Autoencoder\n",
        "  # TODO: set epochs=2000 in final project, 20 is only for draft\n",
        "  def ae(self, dims = [50], epochs=20, batch_size=100, verbose=2, loss='mean_squared_error', latent_act=False, output_act=False, act='relu', patience=20, val_rate=0.2, no_trn=False):\n",
        "\n",
        "    # manipulating an experiment identifier in the output file\n",
        "    if patience != 20:\n",
        "        self.prefix += 'p' + str(patience) + '_'\n",
        "    if len(dims) == 1:\n",
        "        self.prefix += 'AE'\n",
        "    else:\n",
        "        self.prefix += 'DAE'\n",
        "    if loss == 'binary_crossentropy':\n",
        "        self.prefix += 'b'\n",
        "    if latent_act:\n",
        "        self.prefix += 't'\n",
        "    if output_act:\n",
        "        self.prefix += 'T'\n",
        "    self.prefix += str(dims).replace(\", \", \"-\") + '_'\n",
        "    if act == 'sigmoid':\n",
        "        self.prefix = self.prefix + 's'\n",
        "\n",
        "    # filename for temporary model checkpoint\n",
        "    modelName = self.prefix + self.data + '.h5'\n",
        "\n",
        "    # clean up model checkpoint before use\n",
        "    if os.path.isfile(modelName):\n",
        "        os.remove(modelName)\n",
        "\n",
        "    # callbacks for each epoch\n",
        "    callbacks = [EarlyStopping(monitor='val_loss', patience=patience, mode='min', verbose=1),\n",
        "                  ModelCheckpoint(modelName, monitor='val_loss', mode='min', verbose=1, save_best_only=True)]\n",
        "\n",
        "    # spliting the training set into the inner-train and the inner-test set (validation set)\n",
        "    X_inner_train, X_inner_test, y_inner_train, y_inner_test = train_test_split(self.X_train, self.y_train, test_size=val_rate, random_state=self.seed, stratify=self.y_train)\n",
        "\n",
        "    # insert input shape into dimension list\n",
        "    dims.insert(0, X_inner_train.shape[1])\n",
        "\n",
        "    # create autoencoder model\n",
        "    self.autoencoder, self.encoder = autoencoder(dims, act=act, latent_act=latent_act, output_act=output_act)\n",
        "    self.autoencoder.summary()\n",
        "\n",
        "    if no_trn:\n",
        "        return\n",
        "\n",
        "    # compile model\n",
        "    self.autoencoder.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "    # fit model\n",
        "    self.history = self.autoencoder.fit(X_inner_train, X_inner_train, epochs=epochs, batch_size=batch_size, callbacks=callbacks,\n",
        "                          verbose=verbose, validation_data=(X_inner_test, X_inner_test))\n",
        "    # save loss progress\n",
        "    self.saveLossProgress()\n",
        "\n",
        "    # load best model\n",
        "    self.autoencoder = load_model(modelName)\n",
        "    layer_idx = int((len(self.autoencoder.layers) - 1) / 2)\n",
        "    self.encoder = Model(self.autoencoder.layers[0].input, self.autoencoder.layers[layer_idx].output)\n",
        "\n",
        "    # applying the learned encoder into the whole training and the test set.\n",
        "    self.X_train = self.encoder.predict(self.X_train)\n",
        "    self.X_test = self.encoder.predict(self.X_test)\n",
        "\n",
        "  #Shallow Autoencoder & Deep Autoencoder\n",
        "  def ae(self, dims = [50], epochs= 2000, batch_size=100, verbose=2, loss='mean_squared_error', latent_act=False, output_act=False, act='relu', patience=20, val_rate=0.2, no_trn=False):\n",
        "\n",
        "    # manipulating an experiment identifier in the output file\n",
        "    if patience != 20:\n",
        "      self.prefix += 'p' + str(patience) + '_'\n",
        "    if len(dims) == 1:\n",
        "      self.prefix += 'AE'\n",
        "    else:\n",
        "      self.prefix += 'DAE'\n",
        "    if loss == 'binary_crossentropy':\n",
        "      self.prefix += 'b'\n",
        "    if latent_act:\n",
        "      self.prefix += 't'\n",
        "    if output_act:\n",
        "      self.prefix += 'T'\n",
        "    self.prefix += str(dims).replace(\", \", \"-\") + '_'\n",
        "    if act == 'sigmoid':\n",
        "      self.prefix = self.prefix + 's'\n",
        "\n",
        "    # filename for temporary model checkpoint\n",
        "    modelName = self.prefix + self.data + '.h5'\n",
        "\n",
        "    # clean up model checkpoint before use\n",
        "    if os.path.isfile(modelName):\n",
        "      os.remove(modelName)\n",
        "\n",
        "    # callbacks for each epoch\n",
        "    callbacks = [EarlyStopping(monitor='val_loss', patience=patience, mode='min', verbose=1),\n",
        "                  ModelCheckpoint(modelName, monitor='val_loss', mode='min', verbose=1, save_best_only=True)]\n",
        "\n",
        "    # spliting the training set into the inner-train and the inner-test set (validation set)\n",
        "    X_inner_train, X_inner_test, y_inner_train, y_inner_test = train_test_split(self.X_train, self.y_train, test_size=val_rate, random_state=self.seed, stratify=self.y_train)\n",
        "\n",
        "    # insert input shape into dimension list\n",
        "    dims.insert(0, X_inner_train.shape[1])\n",
        "\n",
        "    # create autoencoder model\n",
        "    self.autoencoder, self.encoder = autoencoder(dims, act=act, latent_act=latent_act, output_act=output_act)\n",
        "    self.autoencoder.summary()\n",
        "\n",
        "    if no_trn:\n",
        "      return\n",
        "\n",
        "    # compile model\n",
        "    self.autoencoder.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "    # fit model\n",
        "    self.history = self.autoencoder.fit(X_inner_train, X_inner_train, epochs=epochs, batch_size=batch_size, callbacks=callbacks,\n",
        "                          verbose=verbose, validation_data=(X_inner_test, X_inner_test))\n",
        "    # save loss progress\n",
        "    self.saveLossProgress()\n",
        "\n",
        "    # load best model\n",
        "    self.autoencoder = load_model(modelName)\n",
        "    layer_idx = int((len(self.autoencoder.layers) - 1) / 2)\n",
        "    self.encoder = Model(self.autoencoder.layers[0].input, self.autoencoder.layers[layer_idx].output)\n",
        "\n",
        "    # applying the learned encoder into the whole training and the test set.\n",
        "    self.X_train = self.encoder.predict(self.X_train)\n",
        "    self.X_test = self.encoder.predict(self.X_test)\n",
        "\n",
        "  # Variational Autoencoder\n",
        "  # TODO: set epochs=2000 in final project, 20 is only for draft\n",
        "  def vae(self, dims = [10], epochs=20, batch_size=100, verbose=2, loss='mse', output_act=False, act='relu', patience=25, beta=1.0, warmup=True, warmup_rate=0.01, val_rate=0.2, no_trn=False):\n",
        "\n",
        "    # manipulating an experiment identifier in the output file\n",
        "    if patience != 25:\n",
        "        self.prefix += 'p' + str(patience) + '_'\n",
        "    if warmup:\n",
        "        self.prefix += 'w' + str(warmup_rate) + '_'\n",
        "    self.prefix += 'VAE'\n",
        "    if loss == 'binary_crossentropy':\n",
        "        self.prefix += 'b'\n",
        "    if output_act:\n",
        "        self.prefix += 'T'\n",
        "    if beta != 1:\n",
        "        self.prefix += 'B' + str(beta)\n",
        "    self.prefix += str(dims).replace(\", \", \"-\") + '_'\n",
        "    if act == 'sigmoid':\n",
        "        self.prefix += 'sig_'\n",
        "\n",
        "    # filename for temporary model checkpoint\n",
        "    modelName = self.prefix + self.data + '.h5'\n",
        "\n",
        "    # clean up model checkpoint before use\n",
        "    if os.path.isfile(modelName):\n",
        "        os.remove(modelName)\n",
        "\n",
        "    # callbacks for each epoch\n",
        "    callbacks = [EarlyStopping(monitor='val_loss', patience=patience, mode='min', verbose=1),\n",
        "                  ModelCheckpoint(modelName, monitor='val_loss', mode='min', verbose=1, save_best_only=True,save_weights_only=True)]\n",
        "\n",
        "    # warm-up callback\n",
        "    warm_up_cb = LambdaCallback(on_epoch_end=lambda epoch, logs: [warm_up(epoch)])  # , print(epoch), print(K.get_value(beta))])\n",
        "\n",
        "    # warm-up implementation\n",
        "    def warm_up(epoch):\n",
        "        val = epoch * warmup_rate\n",
        "        if val <= 1.0:\n",
        "            K.set_value(beta, val)\n",
        "    # add warm-up callback if requested\n",
        "    if warmup:\n",
        "        beta = K.variable(value=0.0)\n",
        "        callbacks.append(warm_up_cb)\n",
        "\n",
        "    # spliting the training set into the inner-train and the inner-test set (validation set)\n",
        "    X_inner_train, X_inner_test, y_inner_train, y_inner_test = train_test_split(self.X_train, self.y_train,\n",
        "                                                                                test_size=val_rate,\n",
        "                                                                                random_state=self.seed,\n",
        "                                                                                stratify=self.y_train)\n",
        "\n",
        "    # insert input shape into dimension list\n",
        "    dims.insert(0, X_inner_train.shape[1])\n",
        "\n",
        "    # create vae model\n",
        "    self.vae, self.encoder, self.decoder = variational_AE(dims, act=act, recon_loss=loss, output_act=output_act, beta=beta)\n",
        "    self.vae.summary()\n",
        "\n",
        "    if no_trn:\n",
        "        return\n",
        "\n",
        "    # fit\n",
        "    self.history = self.vae.fit(X_inner_train, epochs=epochs, batch_size=batch_size, callbacks=callbacks, verbose=verbose, validation_data=(X_inner_test, None))\n",
        "\n",
        "    # save loss progress\n",
        "    self.saveLossProgress()\n",
        "\n",
        "    # load best model\n",
        "    self.vae.load_weights(modelName)\n",
        "    self.encoder = self.vae.layers[1]\n",
        "\n",
        "    # applying the learned encoder into the whole training and the test set.\n",
        "    _, _, self.X_train = self.encoder.predict(self.X_train)\n",
        "    _, _, self.X_test = self.encoder.predict(self.X_test)\n",
        "\n",
        "  # Convolutional Autoencoder\n",
        "  # TODO: set epochs=2000 in final project, 20 is only for draft\n",
        "  def cae(self, dims = [32], epochs=20, batch_size=100, verbose=2, loss='mse', output_act=False, act='relu', patience=25, val_rate=0.2, rf_rate = 0.1, st_rate = 0.25, no_trn=False):\n",
        "\n",
        "    # manipulating an experiment identifier in the output file\n",
        "    self.prefix += 'CAE'\n",
        "    if loss == 'binary_crossentropy':\n",
        "        self.prefix += 'b'\n",
        "    if output_act:\n",
        "        self.prefix += 'T'\n",
        "    self.prefix += str(dims).replace(\", \", \"-\") + '_'\n",
        "    if act == 'sigmoid':\n",
        "        self.prefix += 'sig_'\n",
        "\n",
        "    # filename for temporary model checkpoint\n",
        "    modelName = self.prefix + self.data + '.h5'\n",
        "\n",
        "    # clean up model checkpoint before use\n",
        "    if os.path.isfile(modelName):\n",
        "        os.remove(modelName)\n",
        "\n",
        "    # callbacks for each epoch\n",
        "    callbacks = [EarlyStopping(monitor='val_loss', patience=patience, mode='min', verbose=1),\n",
        "                  ModelCheckpoint(modelName, monitor='val_loss', mode='min', verbose=1, save_best_only=True,save_weights_only=True)]\n",
        "\n",
        "\n",
        "    # fill out blank\n",
        "    onesideDim = int(math.sqrt(self.X_train.shape[1])) + 1\n",
        "    enlargedDim = onesideDim ** 2\n",
        "    self.X_train = np.column_stack((self.X_train, np.zeros((self.X_train.shape[0], enlargedDim - self.X_train.shape[1]))))\n",
        "    self.X_test = np.column_stack((self.X_test, np.zeros((self.X_test.shape[0], enlargedDim - self.X_test.shape[1]))))\n",
        "\n",
        "    # reshape\n",
        "    self.X_train = np.reshape(self.X_train, (len(self.X_train), onesideDim, onesideDim, 1))\n",
        "    self.X_test = np.reshape(self.X_test, (len(self.X_test), onesideDim, onesideDim, 1))\n",
        "    self.printDataShapes()\n",
        "\n",
        "    # spliting the training set into the inner-train and the inner-test set (validation set)\n",
        "    X_inner_train, X_inner_test, y_inner_train, y_inner_test = train_test_split(self.X_train, self.y_train,\n",
        "                                                                                test_size=val_rate,\n",
        "                                                                                random_state=self.seed,\n",
        "                                                                                stratify=self.y_train)\n",
        "\n",
        "    # insert input shape into dimension list\n",
        "    dims.insert(0, (onesideDim, onesideDim, 1))\n",
        "\n",
        "    # create cae model\n",
        "    self.cae, self.encoder = conv_autoencoder(dims, act=act, output_act=output_act, rf_rate = rf_rate, st_rate = st_rate)\n",
        "    self.cae.summary()\n",
        "    if no_trn:\n",
        "        return\n",
        "\n",
        "    # compile\n",
        "    self.cae.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "    # fit\n",
        "    self.history = self.cae.fit(X_inner_train, X_inner_train, epochs=epochs, batch_size=batch_size, callbacks=callbacks, verbose=verbose, validation_data=(X_inner_test, X_inner_test, None))\n",
        "\n",
        "    # save loss progress\n",
        "    self.saveLossProgress()\n",
        "\n",
        "    # load best model\n",
        "    self.cae.load_weights(modelName)\n",
        "    if len(self.cae.layers) % 2 == 0:\n",
        "        layer_idx = int((len(self.cae.layers) - 2) / 2)\n",
        "    else:\n",
        "        layer_idx = int((len(self.cae.layers) - 1) / 2)\n",
        "    self.encoder = Model(self.cae.layers[0].input, self.cae.layers[layer_idx].output)\n",
        "\n",
        "    # applying the learned encoder into the whole training and the test set.\n",
        "    self.X_train = self.encoder.predict(self.X_train)\n",
        "    self.X_test = self.encoder.predict(self.X_test)\n",
        "    self.printDataShapes()\n",
        "\n",
        "  # Classification\n",
        "  def classification(self, hyper_parameters, method='svm', cv=5, scoring='roc_auc', n_jobs=1, cache_size=10000):\n",
        "    clf_start_time = time.time()\n",
        "\n",
        "    print(\"# Tuning hyper-parameters\")\n",
        "    print(self.X_train.shape, self.y_train.shape)\n",
        "\n",
        "    # Support Vector Machine\n",
        "    if method == 'svm':\n",
        "      clf = GridSearchCV(SVC(probability=True, cache_size=cache_size), hyper_parameters, cv=StratifiedKFold(cv, shuffle=True), scoring=scoring, n_jobs=n_jobs, verbose=100, )\n",
        "      clf.fit(self.X_train, self.y_train)\n",
        "\n",
        "    # Random Forest\n",
        "    if method == 'rf':\n",
        "      clf = GridSearchCV(RandomForestClassifier(n_jobs=-1, random_state=0), hyper_parameters, cv=StratifiedKFold(cv, shuffle=True), scoring=scoring, n_jobs=n_jobs, verbose=100)\n",
        "      clf.fit(self.X_train, self.y_train)\n",
        "\n",
        "    # Multi-layer Perceptron\n",
        "    if method == 'mlp':\n",
        "      # TODO:  Implement the DNN model and use it here\n",
        "      print(\"mlp classifier todo\")\n",
        "      model = KerasClassifier(build_fn=mlp_model, input_dim=self.X_train.shape[1], verbose=0, )\n",
        "      clf = GridSearchCV(estimator=model, param_grid=hyper_parameters, cv=StratifiedKFold(cv, shuffle=True), scoring=scoring, n_jobs=n_jobs, verbose=100)\n",
        "      clf.fit(self.X_train, self.y_train, batch_size=32)\n",
        "\n",
        "    print(\"Best parameters set found on development set:\")\n",
        "    print()\n",
        "    print(clf.best_params_)\n",
        "\n",
        "    # Evaluate performance of the best model on test set\n",
        "    y_true, y_pred = self.y_test, clf.predict(self.X_test)\n",
        "    y_prob = clf.predict_proba(self.X_test)\n",
        "\n",
        "    # Performance Metrics: AUC, ACC, Recall, Precision, F1_score\n",
        "    metrics = [ round(roc_auc_score(y_true, y_prob[:, 1]), 4),\n",
        "                round(accuracy_score(y_true, y_pred), 4),\n",
        "                round(recall_score(y_true, y_pred), 4),\n",
        "                round(precision_score(y_true, y_pred), 4),\n",
        "                round(f1_score(y_true, y_pred), 4), ]\n",
        "\n",
        "    # time stamp\n",
        "    metrics.append(str(datetime.datetime.now()))\n",
        "\n",
        "    # running time\n",
        "    metrics.append(round( (time.time() - self.t_start), 2))\n",
        "\n",
        "    # classification time\n",
        "    metrics.append(round( (time.time() - clf_start_time), 2))\n",
        "\n",
        "    # best hyper-parameter append\n",
        "    metrics.append(str(clf.best_params_))\n",
        "\n",
        "    # Write performance metrics as a file\n",
        "    # NOTE:  I HAVE DISABLED THIS FOR NOW TO JUST GET SIMPLE RUNNING WORKING, MAYBE LATER\n",
        "\n",
        "    res = pd.DataFrame([metrics], index=[self.prefix + method])\n",
        "    with open(self.data_dir + \"results/\" + self.data + \"_result.txt\", 'a') as f:\n",
        "     res.to_csv(f, header=None)\n",
        "\n",
        "    print('Accuracy metrics')\n",
        "    print('AUC, ACC, Recall, Precision, F1_score, time-end, runtime(sec), classfication time(sec), best hyper-parameter')\n",
        "    print(metrics)\n",
        "\n",
        "  # Print debug info on data shape\n",
        "  def printDataShapes(self, train_only=False):\n",
        "      print(\"X_train.shape: \", self.X_train.shape)\n",
        "      if not train_only:\n",
        "        print(\"y_train.shape: \", self.y_train.shape)\n",
        "        print(\"X_test.shape: \", self.X_test.shape)\n",
        "        print(\"y_test.shape: \", self.y_test.shape)\n",
        "\n",
        "  # ploting loss progress over epochs\n",
        "  def saveLossProgress(self):\n",
        "    #print(self.history.history.keys())\n",
        "    #print(type(self.history.history['loss']))\n",
        "    #print(min(self.history.history['loss']))\n",
        "\n",
        "    loss_collector, loss_max_atTheEnd = self.saveLossProgress_ylim()\n",
        "\n",
        "    # create saving path\n",
        "    if not os.path.exists(os.path.join(self.data_dir, 'results')):\n",
        "      os.mkdir(os.path.join(self.data_dir, 'results'))\n",
        "\n",
        "    # save loss progress - train and val loss only\n",
        "    figureName = self.prefix + self.data + '_' + str(self.seed)\n",
        "    plt.ylim(min(loss_collector)*0.9, loss_max_atTheEnd * 2.0)\n",
        "    plt.plot(self.history.history['loss'])\n",
        "    plt.plot(self.history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train loss', 'val loss'],\n",
        "                loc='upper right')\n",
        "    plt.savefig(self.data_dir + \"results/\" + figureName + '.png')\n",
        "    plt.close()\n",
        "\n",
        "    if 'recon_loss' in self.history.history:\n",
        "        figureName = self.prefix + self.data + '_' + str(self.seed) + '_detailed'\n",
        "        plt.ylim(min(loss_collector) * 0.9, loss_max_atTheEnd * 2.0)\n",
        "        plt.plot(self.history.history['loss'])\n",
        "        plt.plot(self.history.history['val_loss'])\n",
        "        plt.plot(self.history.history['recon_loss'])\n",
        "        plt.plot(self.history.history['val_recon_loss'])\n",
        "        plt.plot(self.history.history['kl_loss'])\n",
        "        plt.plot(self.history.history['val_kl_loss'])\n",
        "        plt.title('model loss')\n",
        "        plt.ylabel('loss')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.legend(['train loss', 'val loss', 'recon_loss', 'val recon_loss', 'kl_loss', 'val kl_loss'], loc='upper right')\n",
        "        plt.savefig(self.data_dir + \"results/\" + figureName + '.png')\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "  # supporting loss plot\n",
        "  def saveLossProgress_ylim(self):\n",
        "    loss_collector = []\n",
        "    loss_max_atTheEnd = 0.0\n",
        "    for hist in self.history.history:\n",
        "      current = self.history.history[hist]\n",
        "      loss_collector += current\n",
        "      if current[-1] >= loss_max_atTheEnd:\n",
        "        loss_max_atTheEnd = current[-1]\n",
        "    return loss_collector, loss_max_atTheEnd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hY7oRbSRldrn"
      },
      "source": [
        "### Run Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jY5P3723sFjE"
      },
      "outputs": [],
      "source": [
        "# main function for running an experiment\n",
        "def run_exp_from_config(config):\n",
        "  try:\n",
        "    if config.exp_design.repeat > 1:\n",
        "      for i in range(config.exp_design.repeat):\n",
        "        run_exp(i, config)\n",
        "    else:\n",
        "      run_exp(config.exp_design.seed, config)\n",
        "  except OSError as error:\n",
        "    print(error)\n",
        "\n",
        "def run_exp(seed, config):\n",
        "\n",
        "\n",
        "  # create an object and load data\n",
        "  ## no argument founded\n",
        "  if config.load_data.data == None and config.load_data.custom_data == None:\n",
        "    print(\"[Error] Please specify an input file. (use -h option for help)\")\n",
        "    exit()\n",
        "  ## provided data\n",
        "  elif config.load_data.data != None:\n",
        "    dm = DeepMicrobiome(data=config.load_data.data + '.txt', seed=seed, data_dir=config.load_data.data_dir)\n",
        "\n",
        "    ## specify feature string\n",
        "    feature_string = ''\n",
        "    data_string = str(config.load_data.data)\n",
        "    if data_string.split('_')[0] == 'abundance':\n",
        "      feature_string = \"k__\"\n",
        "    if data_string.split('_')[0] == 'marker':\n",
        "      feature_string = \"gi|\"\n",
        "\n",
        "    ## load data into the object\n",
        "    dm.loadData(feature_string=feature_string, label_string='disease', label_dict=label_dict,\n",
        "                dtype=dtypeDict[config.load_data.dataType])\n",
        "\n",
        "  ## user data\n",
        "  elif config.load_data.custom_data != None:\n",
        "    # PROBABLY NOT NECESSARY, I'VE COMMENTED IT OUT FOR NOW\n",
        "    \"\"\"\n",
        "    ### without labels - only conducting representation learning\n",
        "    if args.custom_data_labels == None:\n",
        "        dm = DeepMicrobiome(data=args.custom_data, seed=seed, data_dir=args.data_dir)\n",
        "        dm.loadCustomData(dtype=dtypeDict[args.dataType])\n",
        "\n",
        "    ### with labels - conducting representation learning + classification\n",
        "    else:\n",
        "        dm = DeepMicrobiome(data=args.custom_data, seed=seed, data_dir=args.data_dir)\n",
        "        dm.loadCustomDataWithLabels(label_data=args.custom_data_labels, dtype=dtypeDict[args.dataType])\n",
        "    \"\"\"\n",
        "    print(\"custom data currently unsupported.  TODO possibly if needed!\")\n",
        "  else:\n",
        "    exit()\n",
        "\n",
        "  numRLrequired = config.rl.pca + config.rl.ae + config.rl.rp + config.rl.vae + config.rl.cae\n",
        "\n",
        "  if numRLrequired > 1:\n",
        "    raise ValueError('No multiple dimensionality Reduction')\n",
        "\n",
        "  # time check after data has been loaded\n",
        "  dm.t_start = time.time()\n",
        "\n",
        "  # Representation learning (Dimensionality reduction)\n",
        "  if config.rl.pca:\n",
        "    dm.pca()\n",
        "  if config.rl.ae:\n",
        "    dm.ae(dims=[int(i) for i in config.common.dims.split(',')], act=config.common.act, epochs=config.common.max_epochs, loss=config.common.aeloss,\n",
        "          latent_act=config.AE.ae_lact, output_act=config.common.ae_oact, patience=config.common.patience, no_trn=config.others.no_trn)\n",
        "  if config.rl.vae:\n",
        "    dm.vae(dims=[int(i) for i in config.common.dims.split(',')], act=config.common.act, epochs=config.common.max_epochs, loss=config.common.aeloss, output_act=config.common.ae_oact,\n",
        "           patience= 25 if config.common.patience==20 else config.common.patience, beta=config.VAE.vae_beta, warmup=config.VAE.vae_warmup, warmup_rate=config.VAE.vae_warmup_rate, no_trn=config.others.no_trn)\n",
        "  if config.rl.cae:\n",
        "    dm.cae(dims=[int(i) for i in config.common.dims.split(',')], act=config.common.act, epochs=config.common.max_epochs, loss=config.common.aeloss, output_act=config.common.ae_oact,\n",
        "           patience=config.common.patience, rf_rate = config.CAE.rf_rate, st_rate = config.CAE.st_rate, no_trn=config.others.no_trn)\n",
        "  if config.rl.rp:\n",
        "    dm.rp()\n",
        "\n",
        "  # create saving path\n",
        "    if not os.path.exists(os.path.join(dm.data_dir, 'results')):\n",
        "      os.mkdir(os.path.join(dm.data_dir, 'results'))\n",
        "\n",
        "  # write the learned representation of the training set as a file\n",
        "  if config.rl.save_rep:\n",
        "    if numRLrequired == 1:\n",
        "      rep_file = dm.data_dir + \"results/\" + dm.prefix + dm.data + \"_rep.csv\"\n",
        "      pd.DataFrame(dm.X_train).to_csv(rep_file, header=None, index=None)\n",
        "      print(\"The learned representation of the training set has been saved in '{}'\".format(rep_file))\n",
        "    else:\n",
        "      print(\"Warning: Command option '--save_rep' is not applied as no representation learning or dimensionality reduction has been conducted.\")\n",
        "\n",
        "  # Classification\n",
        "  if config.others.no_clf or (config.load_data.data == None and config.load_data.custom_data_labels == None):\n",
        "    print(\"Classification task has been skipped.\")\n",
        "  else:\n",
        "    # turn off GPU\n",
        "    #\n",
        "    # NOTE FROM MATTHEW:  Can we port this over to pytorch?\n",
        "    #\n",
        "    #os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
        "    #importlib.reload(keras)\n",
        "\n",
        "    # training classification models\n",
        "    if config.classification.method == \"svm\":\n",
        "      dm.classification(hyper_parameters=svm_hyper_parameters, method='svm', cv=config.classification.numFolds,\n",
        "                        n_jobs=config.classification.numJobs, scoring=config.classification.scoring, cache_size=config.classification.svm_cache)\n",
        "    elif config.classification.method == \"rf\":\n",
        "      dm.classification(hyper_parameters=rf_hyper_parameters, method='rf', cv=config.classification.numFolds,\n",
        "                        n_jobs=config.classification.numJobs, scoring=config.classification.scoring)\n",
        "    elif config.classification.method == \"mlp\":\n",
        "      dm.classification(hyper_parameters=mlp_hyper_parameters, method='mlp', cv=config.classification.numFolds,\n",
        "                        n_jobs=config.classification.numJobs, scoring=config.classification.scoring)\n",
        "    elif config.classification.method == \"svm_rf\":\n",
        "      dm.classification(hyper_parameters=svm_hyper_parameters, method='svm', cv=config.classification.numFolds,\n",
        "                        n_jobs=config.classification.numJobs, scoring=config.classification.scoring, cache_size=config.classification.svm_cache)\n",
        "      dm.classification(hyper_parameters=rf_hyper_parameters, method='rf', cv=config.classification.numFolds,\n",
        "                        n_jobs=config.classification.numJobs, scoring=config.classification.scoring)\n",
        "    else:\n",
        "      dm.classification(hyper_parameters=svm_hyper_parameters, method='svm', cv=config.classification.numFolds,\n",
        "                        n_jobs=config.classification.numJobs, scoring=config.classification.scoring, cache_size=config.classification.svm_cache)\n",
        "      dm.classification(hyper_parameters=rf_hyper_parameters, method='rf', cv=config.classification.numFolds,\n",
        "                        n_jobs=config.classification.numJobs, scoring=config.classification.scoring)\n",
        "      dm.classification(hyper_parameters=mlp_hyper_parameters, method='mlp', cv=config.classification.numFolds,\n",
        "                        n_jobs=config.classification.numJobs, scoring=config.classification.scoring)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gX6bCcZNuxmz"
      },
      "source": [
        "# Results\n",
        "In this section, you should finish training your model training or loading your trained model. That is a great experiment! You should share the results with others with necessary metrics and figures.\n",
        "\n",
        "Please test and report results for all experiments that you run with:\n",
        "\n",
        "*   specific numbers (accuracy, AUC, RMSE, etc)\n",
        "*   figures (loss shrinkage, outputs from GAN, annotation or label of sample pictures, etc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4WgUnso82d1",
        "outputId": "f4df510a-2b87-43d4-d2fd-68ff9a44bb00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "config ae test\n",
            "loaded data from /content/drive/My Drive/Colab Notebooks/data/abundance/abundance_Cirrhosis.txt\n",
            "X_train.shape:  (185, 10)\n",
            "y_train.shape:  (185,)\n",
            "X_test.shape:  (47, 10)\n",
            "y_test.shape:  (47,)\n",
            "Model: \"AE\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input (InputLayer)          [(None, 10)]              0         \n",
            "                                                                 \n",
            " encoder_0_bottle-neck (Dens  (None, 50)               550       \n",
            " e)                                                              \n",
            "                                                                 \n",
            " decoder_0 (Dense)           (None, 10)                510       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,060\n",
            "Trainable params: 1,060\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 0.03226, saving model to AE[50]_abundance_Cirrhosis.h5\n",
            "2/2 - 1s - loss: 0.1688 - val_loss: 0.0323 - 761ms/epoch - 380ms/step\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 2: val_loss improved from 0.03226 to 0.03014, saving model to AE[50]_abundance_Cirrhosis.h5\n",
            "2/2 - 0s - loss: 0.1599 - val_loss: 0.0301 - 55ms/epoch - 27ms/step\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 3: val_loss improved from 0.03014 to 0.02838, saving model to AE[50]_abundance_Cirrhosis.h5\n",
            "2/2 - 0s - loss: 0.1491 - val_loss: 0.0284 - 74ms/epoch - 37ms/step\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 4: val_loss improved from 0.02838 to 0.02692, saving model to AE[50]_abundance_Cirrhosis.h5\n",
            "2/2 - 0s - loss: 0.1366 - val_loss: 0.0269 - 60ms/epoch - 30ms/step\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 5: val_loss improved from 0.02692 to 0.02552, saving model to AE[50]_abundance_Cirrhosis.h5\n",
            "2/2 - 0s - loss: 0.1317 - val_loss: 0.0255 - 64ms/epoch - 32ms/step\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 6: val_loss improved from 0.02552 to 0.02430, saving model to AE[50]_abundance_Cirrhosis.h5\n",
            "2/2 - 0s - loss: 0.1236 - val_loss: 0.0243 - 63ms/epoch - 31ms/step\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 7: val_loss improved from 0.02430 to 0.02323, saving model to AE[50]_abundance_Cirrhosis.h5\n",
            "2/2 - 0s - loss: 0.1165 - val_loss: 0.0232 - 57ms/epoch - 29ms/step\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 8: val_loss improved from 0.02323 to 0.02225, saving model to AE[50]_abundance_Cirrhosis.h5\n",
            "2/2 - 0s - loss: 0.1095 - val_loss: 0.0223 - 55ms/epoch - 28ms/step\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 9: val_loss improved from 0.02225 to 0.02136, saving model to AE[50]_abundance_Cirrhosis.h5\n",
            "2/2 - 0s - loss: 0.1039 - val_loss: 0.0214 - 85ms/epoch - 42ms/step\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 10: val_loss improved from 0.02136 to 0.02053, saving model to AE[50]_abundance_Cirrhosis.h5\n",
            "2/2 - 0s - loss: 0.0964 - val_loss: 0.0205 - 60ms/epoch - 30ms/step\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 11: val_loss improved from 0.02053 to 0.01971, saving model to AE[50]_abundance_Cirrhosis.h5\n",
            "2/2 - 0s - loss: 0.0928 - val_loss: 0.0197 - 74ms/epoch - 37ms/step\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 12: val_loss improved from 0.01971 to 0.01894, saving model to AE[50]_abundance_Cirrhosis.h5\n",
            "2/2 - 0s - loss: 0.0858 - val_loss: 0.0189 - 54ms/epoch - 27ms/step\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 13: val_loss improved from 0.01894 to 0.01817, saving model to AE[50]_abundance_Cirrhosis.h5\n",
            "2/2 - 0s - loss: 0.0809 - val_loss: 0.0182 - 54ms/epoch - 27ms/step\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 14: val_loss improved from 0.01817 to 0.01741, saving model to AE[50]_abundance_Cirrhosis.h5\n",
            "2/2 - 0s - loss: 0.0774 - val_loss: 0.0174 - 59ms/epoch - 30ms/step\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 15: val_loss improved from 0.01741 to 0.01670, saving model to AE[50]_abundance_Cirrhosis.h5\n",
            "2/2 - 0s - loss: 0.0723 - val_loss: 0.0167 - 60ms/epoch - 30ms/step\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 16: val_loss improved from 0.01670 to 0.01603, saving model to AE[50]_abundance_Cirrhosis.h5\n",
            "2/2 - 0s - loss: 0.0681 - val_loss: 0.0160 - 78ms/epoch - 39ms/step\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 17: val_loss improved from 0.01603 to 0.01539, saving model to AE[50]_abundance_Cirrhosis.h5\n",
            "2/2 - 0s - loss: 0.0629 - val_loss: 0.0154 - 56ms/epoch - 28ms/step\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 18: val_loss improved from 0.01539 to 0.01474, saving model to AE[50]_abundance_Cirrhosis.h5\n",
            "2/2 - 0s - loss: 0.0598 - val_loss: 0.0147 - 78ms/epoch - 39ms/step\n",
            "Epoch 19/20\n",
            "\n",
            "Epoch 19: val_loss improved from 0.01474 to 0.01412, saving model to AE[50]_abundance_Cirrhosis.h5\n",
            "2/2 - 0s - loss: 0.0560 - val_loss: 0.0141 - 80ms/epoch - 40ms/step\n",
            "Epoch 20/20\n",
            "\n",
            "Epoch 20: val_loss improved from 0.01412 to 0.01353, saving model to AE[50]_abundance_Cirrhosis.h5\n",
            "2/2 - 0s - loss: 0.0513 - val_loss: 0.0135 - 81ms/epoch - 40ms/step\n",
            "6/6 [==============================] - 0s 4ms/step\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "The learned representation of the training set has been saved in '/content/drive/My Drive/Colab Notebooks/data/abundance/results/AE[50]_abundance_Cirrhosis_rep.csv'\n",
            "# Tuning hyper-parameters\n",
            "(185, 50) (185,)\n",
            "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
            "[CV 1/5; 1/6] START C=0.25, kernel=linear.......................................\n",
            "[CV 1/5; 1/6] END ........C=0.25, kernel=linear;, score=0.667 total time=   0.0s\n",
            "[CV 2/5; 1/6] START C=0.25, kernel=linear.......................................\n",
            "[CV 2/5; 1/6] END ........C=0.25, kernel=linear;, score=0.668 total time=   0.0s\n",
            "[CV 3/5; 1/6] START C=0.25, kernel=linear.......................................\n",
            "[CV 3/5; 1/6] END ........C=0.25, kernel=linear;, score=0.629 total time=   0.0s\n",
            "[CV 4/5; 1/6] START C=0.25, kernel=linear.......................................\n",
            "[CV 4/5; 1/6] END ........C=0.25, kernel=linear;, score=0.724 total time=   0.0s\n",
            "[CV 5/5; 1/6] START C=0.25, kernel=linear.......................................\n",
            "[CV 5/5; 1/6] END ........C=0.25, kernel=linear;, score=0.719 total time=   0.0s\n",
            "[CV 1/5; 2/6] START C=1, kernel=linear..........................................\n",
            "[CV 1/5; 2/6] END ...........C=1, kernel=linear;, score=0.681 total time=   0.0s\n",
            "[CV 2/5; 2/6] START C=1, kernel=linear..........................................\n",
            "[CV 2/5; 2/6] END ...........C=1, kernel=linear;, score=0.715 total time=   0.0s\n",
            "[CV 3/5; 2/6] START C=1, kernel=linear..........................................\n",
            "[CV 3/5; 2/6] END ...........C=1, kernel=linear;, score=0.614 total time=   0.0s\n",
            "[CV 4/5; 2/6] START C=1, kernel=linear..........................................\n",
            "[CV 4/5; 2/6] END ...........C=1, kernel=linear;, score=0.756 total time=   0.0s\n",
            "[CV 5/5; 2/6] START C=1, kernel=linear..........................................\n",
            "[CV 5/5; 2/6] END ...........C=1, kernel=linear;, score=0.749 total time=   0.0s\n",
            "[CV 1/5; 3/6] START C=0.25, gamma=1, kernel=rbf.................................\n",
            "[CV 1/5; 3/6] END ..C=0.25, gamma=1, kernel=rbf;, score=0.591 total time=   0.0s\n",
            "[CV 2/5; 3/6] START C=0.25, gamma=1, kernel=rbf.................................\n",
            "[CV 2/5; 3/6] END ..C=0.25, gamma=1, kernel=rbf;, score=0.665 total time=   0.0s\n",
            "[CV 3/5; 3/6] START C=0.25, gamma=1, kernel=rbf.................................\n",
            "[CV 3/5; 3/6] END ..C=0.25, gamma=1, kernel=rbf;, score=0.617 total time=   0.0s\n",
            "[CV 4/5; 3/6] START C=0.25, gamma=1, kernel=rbf.................................\n",
            "[CV 4/5; 3/6] END ..C=0.25, gamma=1, kernel=rbf;, score=0.773 total time=   0.0s\n",
            "[CV 5/5; 3/6] START C=0.25, gamma=1, kernel=rbf.................................\n",
            "[CV 5/5; 3/6] END ..C=0.25, gamma=1, kernel=rbf;, score=0.743 total time=   0.0s\n",
            "[CV 1/5; 4/6] START C=0.25, gamma=0.25, kernel=rbf..............................\n",
            "[CV 1/5; 4/6] END C=0.25, gamma=0.25, kernel=rbf;, score=0.655 total time=   0.0s\n",
            "[CV 2/5; 4/6] START C=0.25, gamma=0.25, kernel=rbf..............................\n",
            "[CV 2/5; 4/6] END C=0.25, gamma=0.25, kernel=rbf;, score=0.662 total time=   0.0s\n",
            "[CV 3/5; 4/6] START C=0.25, gamma=0.25, kernel=rbf..............................\n",
            "[CV 3/5; 4/6] END C=0.25, gamma=0.25, kernel=rbf;, score=0.632 total time=   0.0s\n",
            "[CV 4/5; 4/6] START C=0.25, gamma=0.25, kernel=rbf..............................\n",
            "[CV 4/5; 4/6] END C=0.25, gamma=0.25, kernel=rbf;, score=0.756 total time=   0.0s\n",
            "[CV 5/5; 4/6] START C=0.25, gamma=0.25, kernel=rbf..............................\n",
            "[CV 5/5; 4/6] END C=0.25, gamma=0.25, kernel=rbf;, score=0.716 total time=   0.0s\n",
            "[CV 1/5; 5/6] START C=1, gamma=1, kernel=rbf....................................\n",
            "[CV 1/5; 5/6] END .....C=1, gamma=1, kernel=rbf;, score=0.620 total time=   0.0s\n",
            "[CV 2/5; 5/6] START C=1, gamma=1, kernel=rbf....................................\n",
            "[CV 2/5; 5/6] END .....C=1, gamma=1, kernel=rbf;, score=0.703 total time=   0.0s\n",
            "[CV 3/5; 5/6] START C=1, gamma=1, kernel=rbf....................................\n",
            "[CV 3/5; 5/6] END .....C=1, gamma=1, kernel=rbf;, score=0.635 total time=   0.0s\n",
            "[CV 4/5; 5/6] START C=1, gamma=1, kernel=rbf....................................\n",
            "[CV 4/5; 5/6] END .....C=1, gamma=1, kernel=rbf;, score=0.773 total time=   0.0s\n",
            "[CV 5/5; 5/6] START C=1, gamma=1, kernel=rbf....................................\n",
            "[CV 5/5; 5/6] END .....C=1, gamma=1, kernel=rbf;, score=0.781 total time=   0.0s\n",
            "[CV 1/5; 6/6] START C=1, gamma=0.25, kernel=rbf.................................\n",
            "[CV 1/5; 6/6] END ..C=1, gamma=0.25, kernel=rbf;, score=0.661 total time=   0.0s\n",
            "[CV 2/5; 6/6] START C=1, gamma=0.25, kernel=rbf.................................\n",
            "[CV 2/5; 6/6] END ..C=1, gamma=0.25, kernel=rbf;, score=0.665 total time=   0.0s\n",
            "[CV 3/5; 6/6] START C=1, gamma=0.25, kernel=rbf.................................\n",
            "[CV 3/5; 6/6] END ..C=1, gamma=0.25, kernel=rbf;, score=0.629 total time=   0.0s\n",
            "[CV 4/5; 6/6] START C=1, gamma=0.25, kernel=rbf.................................\n",
            "[CV 4/5; 6/6] END ..C=1, gamma=0.25, kernel=rbf;, score=0.753 total time=   0.0s\n",
            "[CV 5/5; 6/6] START C=1, gamma=0.25, kernel=rbf.................................\n",
            "[CV 5/5; 6/6] END ..C=1, gamma=0.25, kernel=rbf;, score=0.746 total time=   0.0s\n",
            "Best parameters set found on development set:\n",
            "\n",
            "{'C': 1, 'kernel': 'linear'}\n",
            "Accuracy metrics\n",
            "AUC, ACC, Recall, Precision, F1_score, time-end, runtime(sec), classfication time(sec), best hyper-parameter\n",
            "[0.6486, 0.4681, 0.9167, 0.4889, 0.6377, '2024-04-14 11:38:50.526819', 4.64, 0.6, \"{'C': 1, 'kernel': 'linear'}\"]\n",
            "# Tuning hyper-parameters\n",
            "(185, 50) (185,)\n",
            "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
            "[CV 1/5; 1/4] START criterion=gini, max_features=sqrt, min_samples_leaf=2, n_estimators=500\n",
            "[CV 1/5; 1/4] END criterion=gini, max_features=sqrt, min_samples_leaf=2, n_estimators=500;, score=0.687 total time=   1.4s\n",
            "[CV 2/5; 1/4] START criterion=gini, max_features=sqrt, min_samples_leaf=2, n_estimators=500\n",
            "[CV 2/5; 1/4] END criterion=gini, max_features=sqrt, min_samples_leaf=2, n_estimators=500;, score=0.839 total time=   1.4s\n",
            "[CV 3/5; 1/4] START criterion=gini, max_features=sqrt, min_samples_leaf=2, n_estimators=500\n",
            "[CV 3/5; 1/4] END criterion=gini, max_features=sqrt, min_samples_leaf=2, n_estimators=500;, score=0.725 total time=   1.4s\n",
            "[CV 4/5; 1/4] START criterion=gini, max_features=sqrt, min_samples_leaf=2, n_estimators=500\n",
            "[CV 4/5; 1/4] END criterion=gini, max_features=sqrt, min_samples_leaf=2, n_estimators=500;, score=0.789 total time=   1.3s\n",
            "[CV 5/5; 1/4] START criterion=gini, max_features=sqrt, min_samples_leaf=2, n_estimators=500\n",
            "[CV 5/5; 1/4] END criterion=gini, max_features=sqrt, min_samples_leaf=2, n_estimators=500;, score=0.725 total time=   1.3s\n",
            "[CV 1/5; 2/4] START criterion=gini, max_features=log2, min_samples_leaf=2, n_estimators=500\n",
            "[CV 1/5; 2/4] END criterion=gini, max_features=log2, min_samples_leaf=2, n_estimators=500;, score=0.678 total time=   1.2s\n",
            "[CV 2/5; 2/4] START criterion=gini, max_features=log2, min_samples_leaf=2, n_estimators=500\n",
            "[CV 2/5; 2/4] END criterion=gini, max_features=log2, min_samples_leaf=2, n_estimators=500;, score=0.836 total time=   1.3s\n",
            "[CV 3/5; 2/4] START criterion=gini, max_features=log2, min_samples_leaf=2, n_estimators=500\n",
            "[CV 3/5; 2/4] END criterion=gini, max_features=log2, min_samples_leaf=2, n_estimators=500;, score=0.737 total time=   1.3s\n",
            "[CV 4/5; 2/4] START criterion=gini, max_features=log2, min_samples_leaf=2, n_estimators=500\n",
            "[CV 4/5; 2/4] END criterion=gini, max_features=log2, min_samples_leaf=2, n_estimators=500;, score=0.789 total time=   1.6s\n",
            "[CV 5/5; 2/4] START criterion=gini, max_features=log2, min_samples_leaf=2, n_estimators=500\n",
            "[CV 5/5; 2/4] END criterion=gini, max_features=log2, min_samples_leaf=2, n_estimators=500;, score=0.737 total time=   1.6s\n",
            "[CV 1/5; 3/4] START criterion=entropy, max_features=sqrt, min_samples_leaf=2, n_estimators=500\n",
            "[CV 1/5; 3/4] END criterion=entropy, max_features=sqrt, min_samples_leaf=2, n_estimators=500;, score=0.678 total time=   1.6s\n",
            "[CV 2/5; 3/4] START criterion=entropy, max_features=sqrt, min_samples_leaf=2, n_estimators=500\n",
            "[CV 2/5; 3/4] END criterion=entropy, max_features=sqrt, min_samples_leaf=2, n_estimators=500;, score=0.836 total time=   1.3s\n",
            "[CV 3/5; 3/4] START criterion=entropy, max_features=sqrt, min_samples_leaf=2, n_estimators=500\n",
            "[CV 3/5; 3/4] END criterion=entropy, max_features=sqrt, min_samples_leaf=2, n_estimators=500;, score=0.728 total time=   1.4s\n",
            "[CV 4/5; 3/4] START criterion=entropy, max_features=sqrt, min_samples_leaf=2, n_estimators=500\n",
            "[CV 4/5; 3/4] END criterion=entropy, max_features=sqrt, min_samples_leaf=2, n_estimators=500;, score=0.787 total time=   1.2s\n",
            "[CV 5/5; 3/4] START criterion=entropy, max_features=sqrt, min_samples_leaf=2, n_estimators=500\n",
            "[CV 5/5; 3/4] END criterion=entropy, max_features=sqrt, min_samples_leaf=2, n_estimators=500;, score=0.728 total time=   1.2s\n",
            "[CV 1/5; 4/4] START criterion=entropy, max_features=log2, min_samples_leaf=2, n_estimators=500\n",
            "[CV 1/5; 4/4] END criterion=entropy, max_features=log2, min_samples_leaf=2, n_estimators=500;, score=0.681 total time=   1.3s\n",
            "[CV 2/5; 4/4] START criterion=entropy, max_features=log2, min_samples_leaf=2, n_estimators=500\n",
            "[CV 2/5; 4/4] END criterion=entropy, max_features=log2, min_samples_leaf=2, n_estimators=500;, score=0.839 total time=   1.3s\n",
            "[CV 3/5; 4/4] START criterion=entropy, max_features=log2, min_samples_leaf=2, n_estimators=500\n",
            "[CV 3/5; 4/4] END criterion=entropy, max_features=log2, min_samples_leaf=2, n_estimators=500;, score=0.725 total time=   1.3s\n",
            "[CV 4/5; 4/4] START criterion=entropy, max_features=log2, min_samples_leaf=2, n_estimators=500\n",
            "[CV 4/5; 4/4] END criterion=entropy, max_features=log2, min_samples_leaf=2, n_estimators=500;, score=0.798 total time=   1.5s\n",
            "[CV 5/5; 4/4] START criterion=entropy, max_features=log2, min_samples_leaf=2, n_estimators=500\n",
            "[CV 5/5; 4/4] END criterion=entropy, max_features=log2, min_samples_leaf=2, n_estimators=500;, score=0.740 total time=   1.7s\n",
            "Best parameters set found on development set:\n",
            "\n",
            "{'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 2, 'n_estimators': 500}\n",
            "Accuracy metrics\n",
            "AUC, ACC, Recall, Precision, F1_score, time-end, runtime(sec), classfication time(sec), best hyper-parameter\n",
            "[0.6812, 0.617, 0.625, 0.625, 0.625, '2024-04-14 11:39:19.903562', 34.02, 29.37, \"{'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 2, 'n_estimators': 500}\"]\n",
            "# Tuning hyper-parameters\n",
            "(185, 50) (185,)\n",
            "mlp classifier todo\n",
            "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
            "[CV 1/5; 1/2] START dropout_rate=0.1, epochs=30, numHiddenLayers=1, numUnits=10.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-9-752cfe8a250a>:420: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  model = KerasClassifier(build_fn=mlp_model, input_dim=self.X_train.shape[1], verbose=0, )\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 5ms/step\n",
            "[CV 1/5; 1/2] END dropout_rate=0.1, epochs=30, numHiddenLayers=1, numUnits=10;, score=0.757 total time=   1.2s\n",
            "[CV 2/5; 1/2] START dropout_rate=0.1, epochs=30, numHiddenLayers=1, numUnits=10.\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "[CV 2/5; 1/2] END dropout_rate=0.1, epochs=30, numHiddenLayers=1, numUnits=10;, score=0.819 total time=   1.2s\n",
            "[CV 3/5; 1/2] START dropout_rate=0.1, epochs=30, numHiddenLayers=1, numUnits=10.\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "[CV 3/5; 1/2] END dropout_rate=0.1, epochs=30, numHiddenLayers=1, numUnits=10;, score=0.588 total time=   1.6s\n",
            "[CV 4/5; 1/2] START dropout_rate=0.1, epochs=30, numHiddenLayers=1, numUnits=10.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 15 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7c4cb5e7da20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 8ms/step\n",
            "[CV 4/5; 1/2] END dropout_rate=0.1, epochs=30, numHiddenLayers=1, numUnits=10;, score=0.655 total time=   1.3s\n",
            "[CV 5/5; 1/2] START dropout_rate=0.1, epochs=30, numHiddenLayers=1, numUnits=10.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 17 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7c4cb5e7f5b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 7ms/step\n",
            "[CV 5/5; 1/2] END dropout_rate=0.1, epochs=30, numHiddenLayers=1, numUnits=10;, score=0.661 total time=   1.3s\n",
            "[CV 1/5; 2/2] START dropout_rate=0.3, epochs=30, numHiddenLayers=1, numUnits=10.\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "[CV 1/5; 2/2] END dropout_rate=0.3, epochs=30, numHiddenLayers=1, numUnits=10;, score=0.558 total time=   1.7s\n",
            "[CV 2/5; 2/2] START dropout_rate=0.3, epochs=30, numHiddenLayers=1, numUnits=10.\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "[CV 2/5; 2/2] END dropout_rate=0.3, epochs=30, numHiddenLayers=1, numUnits=10;, score=0.749 total time=   1.2s\n",
            "[CV 3/5; 2/2] START dropout_rate=0.3, epochs=30, numHiddenLayers=1, numUnits=10.\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "[CV 3/5; 2/2] END dropout_rate=0.3, epochs=30, numHiddenLayers=1, numUnits=10;, score=0.427 total time=   1.7s\n",
            "[CV 4/5; 2/2] START dropout_rate=0.3, epochs=30, numHiddenLayers=1, numUnits=10.\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "[CV 4/5; 2/2] END dropout_rate=0.3, epochs=30, numHiddenLayers=1, numUnits=10;, score=0.732 total time=   1.8s\n",
            "[CV 5/5; 2/2] START dropout_rate=0.3, epochs=30, numHiddenLayers=1, numUnits=10.\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "[CV 5/5; 2/2] END dropout_rate=0.3, epochs=30, numHiddenLayers=1, numUnits=10;, score=0.678 total time=   1.3s\n",
            "Best parameters set found on development set:\n",
            "\n",
            "{'dropout_rate': 0.1, 'epochs': 30, 'numHiddenLayers': 1, 'numUnits': 10}\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "Accuracy metrics\n",
            "AUC, ACC, Recall, Precision, F1_score, time-end, runtime(sec), classfication time(sec), best hyper-parameter\n",
            "[0.5906, 0.5319, 0.9167, 0.5238, 0.6667, '2024-04-14 11:39:35.578407', 49.7, 15.67, \"{'dropout_rate': 0.1, 'epochs': 30, 'numHiddenLayers': 1, 'numUnits': 10}\"]\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nconfig ae test\")\n",
        "config1 = DeepMicro_Config(\"ae_config\")\n",
        "run_exp_from_config(config1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "2sMiKPrrArc9",
        "outputId": "f633619d-db7a-434f-c9f9-daf4fa750c19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "config cae test\n",
            "loaded data from /content/drive/My Drive/Colab Notebooks/data/abundance/abundance_Cirrhosis.txt\n",
            "X_train.shape:  (185, 10)\n",
            "y_train.shape:  (185,)\n",
            "X_test.shape:  (47, 10)\n",
            "y_test.shape:  (47,)\n",
            "X_train.shape:  (185, 4, 4, 1)\n",
            "y_train.shape:  (185,)\n",
            "X_test.shape:  (47, 4, 4, 1)\n",
            "y_test.shape:  (47,)\n",
            "[(4, 4, 1), 50]\n",
            "receptive field (kernel) size: 0\n",
            "stride size: 1\n",
            "heello 50 glorot_uniform\n",
            "rf_size: 0, st_size: 1\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "The `kernel_size` argument must be a tuple of 2 integers. Received: (0, 0) including {0} that does not satisfy the requirement `> 0`.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-5fc64effe29c>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nconfig cae test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mconfig2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeepMicro_Config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cae_config\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrun_exp_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-babc2d18f350>\u001b[0m in \u001b[0;36mrun_exp_from_config\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mrun_exp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m       \u001b[0mrun_exp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp_design\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-babc2d18f350>\u001b[0m in \u001b[0;36mrun_exp\u001b[0;34m(seed, config)\u001b[0m\n\u001b[1;32m     70\u001b[0m            patience= 25 if config.common.patience==20 else config.common.patience, beta=config.VAE.vae_beta, warmup=config.VAE.vae_warmup, warmup_rate=config.VAE.vae_warmup_rate, no_trn=config.others.no_trn)\n\u001b[1;32m     71\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcae\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     dm.cae(dims=[int(i) for i in config.common.dims.split(',')], act=config.common.act, epochs=config.common.max_epochs, loss=config.common.aeloss, output_act=config.common.ae_oact,\n\u001b[0m\u001b[1;32m     73\u001b[0m            patience=config.common.patience, rf_rate = config.CAE.rf_rate, st_rate = config.CAE.st_rate, no_trn=config.others.no_trn)\n\u001b[1;32m     74\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-752cfe8a250a>\u001b[0m in \u001b[0;36mcae\u001b[0;34m(self, dims, epochs, batch_size, verbose, loss, output_act, act, patience, val_rate, rf_rate, st_rate, no_trn)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;31m# create cae model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_autoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_act\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_act\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrf_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mst_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mst_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mno_trn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-be6a6f452674>\u001b[0m in \u001b[0;36mconv_autoencoder\u001b[0;34m(dims, act, init, latent_act, output_act, rf_rate, st_rate)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_internal_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rf_size: %d, st_size: %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrf_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         h = Conv2D(\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mdims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrf_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrf_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstride_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/dtensor/utils.py\u001b[0m in \u001b[0;36m_wrap_function\u001b[0;34m(layer_instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m                     \u001b[0mlayout_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvariable_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_layout\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0minit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_instance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;31m# Inject the layout parameter after the invocation of __init__()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/layers/convolutional/conv2d.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filters, kernel_size, strides, padding, data_format, dilation_rate, groups, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     ):\n\u001b[0;32m--> 179\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mrank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/layers/convolutional/base_conv.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, rank, filters, kernel_size, strides, padding, data_format, dilation_rate, groups, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, conv_op, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         self.kernel_size = conv_utils.normalize_tuple(\n\u001b[0m\u001b[1;32m    137\u001b[0m             \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"kernel_size\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/conv_utils.py\u001b[0m in \u001b[0;36mnormalize_tuple\u001b[0;34m(value, n, name, allow_zero)\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;34mf\" that does not satisfy the requirement `{req_msg}`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         )\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalue_tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The `kernel_size` argument must be a tuple of 2 integers. Received: (0, 0) including {0} that does not satisfy the requirement `> 0`."
          ]
        }
      ],
      "source": [
        "print(\"\\nconfig cae test\")\n",
        "config2 = DeepMicro_Config(\"cae_config\")\n",
        "run_exp_from_config(config2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjW9bCkouv8O"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EAWAy_LwHlV"
      },
      "source": [
        "## Model comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH75TNU71eRH"
      },
      "source": [
        "# Discussion\n",
        "\n",
        "In this section,you should discuss your work and make future plan. The discussion should address the following questions:\n",
        "  * Make assessment that the paper is reproducible or not.\n",
        "  * Explain why it is not reproducible if your results are kind negative.\n",
        "  * Describe “What was easy” and “What was difficult” during the reproduction.\n",
        "  * Make suggestions to the author or other reproducers on how to improve the reproducibility.\n",
        "  * What will you do in next phase.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2VDXo5F4Frm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHMI2chl9omn"
      },
      "source": [
        "# References\n",
        "\n",
        "1. Cho, I., & Blaser, M. J. (2012). The human microbiome: at the interface of health and disease. Nature Reviews Genetics, 13(4), 260-270.\n",
        "2. Huttenhower, C. et al. Structure, function and diversity of the healthy human microbiome. nature 486, 207 (2012).\n",
        "3. McQuade, J. L., Daniel, C. R., Helmink, B. A. & Wargo, J. A. Modulating the microbiome to improve therapeutic response in cancer. The Lancet Oncology 20, e77–e91 (2019).\n",
        "4. Eloe-Fadrosh, E. A. & Rasko, D. A. The human microbiome: from symbiosis to pathogenesis. Annual review of medicine 64, 145–163 (2013).\n",
        "5. Scholz, M. et al. Strain-level microbial epidemiology and population genomics from shotgun metagenomics. Nature methods 13, 435 (2016).\n",
        "6. Truong, D. T. et al. MetaPhlAn2 for enhanced metagenomic taxonomic profiling. Nature methods 12, 902 (2015).\n",
        "7. Oh, M., & Zhang, L. (2020). DeepMicro: deep representation learning for disease prediction based on microbiome data. Scientific reports, 10(1), 6026.\n",
        "8. Pasolli, E., Truong, D. T., Malik, F., Waldron, L. & Segata, N. Machine learning meta-analysis of large metagenomic datasets: tools and biological insights. PLoS computational biology 12, e1004977 (2016).\n",
        "9. Cawley, G. C. & Talbot, N. L. On over-fitting in model selection and subsequent selection bias in performance evaluation. Journal of Machine Learning Research 11, 2079–2107 (2010).\n",
        "10. Varma, S. & Simon, R. Bias in error estimation when using cross-validation for model selection. BMC bioinformatics 7, 91 (2006).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j87SK6_CsmOe"
      },
      "outputs": [],
      "source": [
        "# test a full (simple) workflow with PCA and SVM\n",
        "print(\"\\nconfig 1 test\")\n",
        "config1 = DeepMicro_Config(\"test_experiment_config_1\")\n",
        "run_exp_from_config(config1)\n",
        "\n",
        "# Note: If you're lazy, you don't have to create a whole new config file when you're just changing one or two parameters.\n",
        "# you can run a slightly modified experiment like this (e.g. here we're using the same exeriment on a different dataset):\n",
        "config1_using_marker_colorectal = config2\n",
        "config1_using_marker_colorectal.load_data.data_dir = \"/content/drive/My Drive/Colab Notebooks/data/marker/\"\n",
        "config1_using_marker_colorectal.load_data.data = \"marker_Colorectal\"\n",
        "run_exp_from_config(config1_using_marker_colorectal)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
